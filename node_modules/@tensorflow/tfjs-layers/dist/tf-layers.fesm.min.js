/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
import{backend as t,util as e,serialization as n,tidy as s,sqrt as i,sum as r,mul as a,clipByValue as o,div as l,add as u,relu as h,slice as c,slice4d as p,slice3d as d,slice2d as f,slice1d as g,tensor1d as m,gather as y,tile as b,randomNormal as w,elu as k,abs as x,dropout as v,concat as S,fused as I,concat4d as N,concat3d as A,concat2d as C,concat1d as z,zeros as D,ones as $,scalar as T,randomUniform as E,truncatedNormal as F,eye as _,linalg as L,variable as R,dispose as M,nextFrame as O,keep as B,log as P,sub as W,mean as U,softmax as K,neg as j,fill as V,maximum as q,floor as G,oneHot as H,max as J,softplus as Z,onesLike as Y,greater as X,equal as Q,argMax as tt,logicalAnd as et,where as nt,train as st,memory as it,cast as rt,Tensor as at,Optimizer as ot,io as lt,selu as ut,minimum as ht,sigmoid as ct,tanh as pt,logSoftmax as dt,leakyRelu as ft,prelu as gt,transpose as mt,conv2dTranspose as yt,conv3dTranspose as bt,conv1d as wt,conv3d as kt,separableConv2d as xt,depthwiseConv2d as vt,expandDims as St,reverse as It,unstack as Nt,stack as At,split as Ct,conv2d as zt,any as Dt,notEqual as $t,zerosLike as Tt,all as Et,greaterEqual as Ft,moments as _t,batchNorm2d as Lt,batchNorm3d as Rt,batchNorm4d as Mt,pad as Ot,maxPool as Bt,avgPool as Pt,maxPool3d as Wt,avgPool3d as Ut,squeeze as Kt}from"@tensorflow/tfjs-core";function jt(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function Vt(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function qt(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||Xt(t)&&!n)for(let s=0;s<t.length;++s)qt(t[s],e,n);else e.push(t);return e}function Gt(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function Ht(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function Jt(t){return t%1==0}function Zt(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function Yt(t,e){const n=e.length;return Vt((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),Vt(t.every((t=>Jt(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function Xt(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function Qt(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function te(t){return"string"==typeof t||t instanceof String}function ee(t){return Array.isArray(t)?ee(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":te(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function ne(t){return!!(t&&t.constructor&&t.call&&t.apply)}function se(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function ie(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=ie(t+e*o,a,n,s)}return i}function re(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return ie(0,t,e,n)}function ae(t,e){const n=oe(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function oe(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function le(t){return t&&t.then&&"function"==typeof t.then}class ue{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=he,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(le(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if("tfjsflags"in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function he(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function ce(){return de}let pe,de=null;function fe(){if(null==pe){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}pe=t}return pe}function ge(t,e){const n=function(){const t=fe();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const me=ge("kernelRegistry",(()=>new Map)),ye=ge("gradRegistry",(()=>new Map));function be(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return me.get(n)}function we(t){return ye.get(t)}function ke(t){const e=me.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function xe(t){const{kernelName:e}=t;ye.has(e)&&ce().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),ye.set(e,t)}function ve(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=qt(t)),ce().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function Se(){return ce().platform.now()}function Ie(t,e="utf-8"){return e=e||"utf-8",ce().platform.decode(t,e)}class Ne{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new Ce)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=Se();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:Se()-a})}if(ce().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{Ae(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function Ae(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class Ce{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?Zt(`${s}ms`,9):s.error,o=Zt(t,25),l=e.rank,u=e.size,h=Zt(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function ze(t,e,n,s){const i=se(e),r=function(t,e,n,s){const i=Gt(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?Ee(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],De(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=Te(t,e,n,i,r),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${a}`),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map((t=>"    "+t)).join("\n")),l.join("\n")}function De(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:te(t)?`'${t}'`:"bool"===n?$e(t):parseFloat(t.toFixed(7)).toString(),Zt(s,e)}function $e(t){return 0===t?"false":"true"}function Te(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[De(Ee(t)[0],0,n)]}return"bool"===n?[$e(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=Ee(s),r=Ee(r)),["["+s.map(((t,e)=>De(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>De(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?Ee(t):Array.from(t)).map(((t,e)=>De(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...Te(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...Te(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...Te(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function Ee(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Fe=null;class _e{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=Gt(t),this.strides=se(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return null.buffer(this.shape,this.dtype,t)}bufferSync(){return null.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return re(this.shape,t,"complex64"===this.dtype)}arraySync(){return re(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=Fe().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>Ie(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Fe().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>Ie(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Fe().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Fe().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return null.print(this,t)}clone(){return this.throwIfDisposed(),null.clone(this)}toString(t=!1){return ze(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),null.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Fe().makeVariable(this,t,e,n)}}function Le(){return ge("Tensor",(()=>_e))}Object.defineProperty(_e,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),Le();class Re extends _e{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!Ht(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Fe().disposeTensor(this),this.dataId=t.dataId,Fe().incRef(this,null)}dispose(){Fe().disposeVariable(this),this.isDisposedInternal=!0}}var Me,Oe,Be,Pe,We;Object.defineProperty(Re,Symbol.hasInstance,{value:t=>t instanceof _e&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(Me||(Me={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(Oe||(Oe={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(Be||(Be={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(Pe||(Pe={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(We||(We={}));const Ue={float32:Pe,int32:Oe,bool:Be,complex64:We};function Ke(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Ue[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function je(t){const e=[];return Ve(t,e,new Set),e}function Ve(t,e,n){if(null==t)return;if(t instanceof _e)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),Ve(s,e,n))}}function qe(t){return null!=t.kernelName}class Ge{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class He{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new Ge}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new Ne(this.backendInstance),!0}setupRegisteredKernels(){ke(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){ke(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{refCount(t){return jt("refCount")}incRef(t){return jt("incRef")}timerAvailable(){return!0}time(t){return jt("time")}read(t){return jt("read")}readSync(t){return jt("readSync")}numDataIds(){return jt("numDataIds")}disposeData(t,e){return jt("disposeData")}write(t,e,n){return jt("write")}move(t,e,n,s,i){return jt("move")}memory(){return jt("memory")}floatPrecision(){return jt("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return jt("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return He.nextTensorId++}nextVariableId(){return He.nextVariableId++}clone(t){const e=Je.runKernel("Identity",{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Je.runKernel("Cast",e,n)}})),[],{}),e}runKernel(t,e,n){if(!(null!=be(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=qe(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(qe(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=be(e,this.backendName);Vt(null!=l,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map((t=>this.keep(this.clone(t)))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy((()=>e(this.backend,i)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=qe(t)?null:t.backwardsFunc;let p;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,(()=>a())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()})),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map((t=>null!=u[t]?u[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const s=we(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(Vt(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),r=Object.keys(e).map((t=>e[t]))):r=t.map((t=>e[t]));const a=n.filter(((t,e)=>i[e]));return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&te(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",ce().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new _e(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new _e(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new Re(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*Qt(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof Re||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*Qt(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=we(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=oe(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=je(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,s=!1){if(Vt(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));Vt(i instanceof _e,(()=>"The result y returned by f() must be a tensor."));const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[i.id]=null==n?function(t){const e=ae(Gt(t),"float32");return Je.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n((()=>o[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!Ht(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,(t=>this.tidy(t)),Ze);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:i,grads:s}}))}customGrad(t){return Vt(ne(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;Vt(e.every((t=>t instanceof _e)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};e.forEach(((t,e)=>{s[e]=t}));return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),Vt(n.value instanceof _e,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),Vt(ne(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];Vt(r.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),Vt(r.every((t=>t instanceof _e)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const a={};return r.forEach(((t,e)=>{a[e]=()=>t})),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=Se(),n=await this.backend.time(t);return n.wallMs=Se()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new Ge;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}He.nextTensorId=0,He.nextVariableId=0;const Je=function(){const t=fe();if(null==t._tfengine){const e=new ue(t);t._tfengine=new He(e)}var e;return e=t._tfengine.ENV,de=e,Fe=()=>t._tfengine,t._tfengine}();function Ze(t,e){const n={a:t,b:e};return Je.runKernel("Add",n)}function Ye(t,e){let n=t;if(Xt(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||Xt(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&ce().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&Xe(t,s,[]),s}function Xe(t,e,n){if(n=n||[],!Array.isArray(t)&&!Xt(t))return void Vt(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));Vt(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),Vt(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const s=e.slice(1);for(let e=0;e<t.length;++e)Xe(t[e],s,n.concat(e))}function Qe(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function tn(t,e,n,s="numeric"){if(t instanceof _e)return Qe(s,t.dtype,e,n),t;let i=ee(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),Qe(s,i,e,n),null==t||!Xt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=Ye(t,i);Xt(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?ve(t,i):qt(t,[],!0);return Je.makeTensor(a,r,i)}function en(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>tn(t,`${e}[${i}]`,n,s)))}function nn(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Je.startScope(n);try{const e=s(...t);return le(e)&&console.error("Cannot return a Promise inside of tidy."),Je.endScope(e),e}catch(t){throw Je.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const sn=nn({abs_:function(t){const e=tn(t,"x","abs");if("complex64"===e.dtype){const t={x:e};return Je.runKernel("ComplexAbs",t)}{const t={x:e};return Je.runKernel("Abs",t)}}});const rn=nn({acos_:function(t){const e={x:tn(t,"x","acos")};return Je.runKernel("Acos",e)}});const an=nn({acosh_:function(t){const e={x:tn(t,"x","acosh")};return Je.runKernel("Acosh",e)}});const on=nn({add_:function(t,e){let n=tn(t,"a","add"),s=tn(e,"b","add");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Add",i)}});const ln=nn({all_:function(t,e=null,n=!1){const s={x:tn(t,"x","all","bool")},i={axis:e,keepDims:n};return Je.runKernel("All",s,i)}});const un=nn({any_:function(t,e=null,n=!1){const s={x:tn(t,"x","any","bool")},i={axis:e,keepDims:n};return Je.runKernel("Any",s,i)}});const hn=nn({argMax_:function(t,e=0){const n={x:tn(t,"x","argMax")},s={axis:e};return Je.runKernel("ArgMax",n,s)}});const cn=nn({argMin_:function(t,e=0){const n={x:tn(t,"x","argMin")},s={axis:e};return Je.runKernel("ArgMin",n,s)}});const pn=nn({asin_:function(t){const e={x:tn(t,"x","asin")};return Je.runKernel("Asin",e)}});const dn=nn({asinh_:function(t){const e={x:tn(t,"x","asinh")};return Je.runKernel("Asinh",e)}});const fn=nn({atan_:function(t){const e={x:tn(t,"x","atan")};return Je.runKernel("Atan",e)}});const gn=nn({atan2_:function(t,e){let n=tn(t,"a","atan2"),s=tn(e,"b","atan2");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Atan2",i)}});const mn=nn({atanh_:function(t){const e={x:tn(t,"x","atanh")};return Je.runKernel("Atanh",e)}});const yn=nn({cast_:function(t,e){const n=tn(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Je.runKernel("Cast",s,i)}});function bn(t,e,n,s,i,r,a="channelsLast"){const[o,l]=wn(e);let u;if("channelsLast"===a)u=[o,l,t[3],t[3]];else{if("channelsFirst"!==a)throw new Error(`Unknown dataFormat ${a}`);u=[o,l,t[1],t[1]]}return function(t,e,n,s,i,r,a=!1,o="channelsLast"){let[l,u,h,c]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,h,c]=t;else{if("channelsFirst"!==o)throw new Error(`Unknown dataFormat ${o}`);[l,c,u,h]=t}const[p,d,,f]=e,[g,m]=wn(n),[y,b]=wn(s),w=kn(p,y),k=kn(d,b),{padInfo:x,outHeight:v,outWidth:S}=function(t,e,n,s,i,r,a,o,l){let u,h,c;if("number"==typeof t){u={top:t,bottom:t,left:t,right:t,type:0===t?"VALID":"NUMBER"};const i=function(t,e,n,s,i){null==s&&(s=function(t,e,n,s=1){const i=kn(e,s);return Math.floor((t[0]*(n-1)-n+i)/2)}(t,e,n));const r=t[0],a=t[1],o=xn((r-e+2*s)/n+1,i),l=xn((a-e+2*s)/n+1,i);return[o,l]}([e,n],r,s,t,o);h=i[0],c=i[1]}else if("same"===t){h=Math.ceil(e/s),c=Math.ceil(n/i);const t=Math.max(0,(h-1)*s+r-e),o=Math.max(0,(c-1)*i+a-n),l=Math.floor(t/2),p=t-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===t)u={top:0,bottom:0,left:0,right:0,type:"VALID"},h=Math.ceil((e-r+1)/s),c=Math.ceil((n-a+1)/i);else{if("object"!=typeof t)throw Error(`Unknown padding parameter: ${t}`);{const p="channelsLast"===l?t[1][0]:t[2][0],d="channelsLast"===l?t[1][1]:t[2][1],f="channelsLast"===l?t[2][0]:t[3][0],g="channelsLast"===l?t[2][1]:t[3][1];u={top:p,bottom:d,left:f,right:g,type:0===p&&0===d&&0===f&&0===g?"VALID":"EXPLICIT"},h=xn((e-r+p+d)/s+1,o),c=xn((n-a+f+g)/i+1,o)}}return{padInfo:u,outHeight:h,outWidth:c}}(i,u,h,g,m,w,k,r,o),I=a?f*c:f;let N;"channelsFirst"===o?N=[l,I,v,S]:"channelsLast"===o&&(N=[l,v,S,I]);return{batchSize:l,dataFormat:o,inHeight:u,inWidth:h,inChannels:c,outHeight:v,outWidth:S,outChannels:I,padInfo:x,strideHeight:g,strideWidth:m,filterHeight:p,filterWidth:d,effectiveFilterHeight:w,effectiveFilterWidth:k,dilationHeight:y,dilationWidth:b,inShape:t,outShape:N,filterShape:e}}(t,u,n,s,i,r,!1,a)}function wn(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}function kn(t,e){return e<=1?t:t+(t-1)*(e-1)}function xn(t,e){if(!e)return Math.trunc(t);switch(e){case"round":return Math.round(t);case"ceil":return Math.ceil(t);case"floor":return Math.floor(t);default:throw new Error(`Unknown roundingMode ${e}`)}}function vn(t){const[e,n,s]=wn(t);return 1===e&&1===n&&1===s}function Sn(t,e){return vn(t)||vn(e)}const In=nn({reshape_:function(t,e){const n={x:tn(t,"x","reshape","string_or_numeric")},s={shape:e};return Je.runKernel("Reshape",n,s)}});const Nn=nn({avgPool_:function(t,e,n,s,i){const r=tn(t,"x","avgPool","float32");Vt(Sn(n,1),(()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`));let a=r,o=!1;3===r.rank&&(o=!0,a=In(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===a.rank,(()=>`Error in avgPool: x must be rank 4 but got rank ${a.rank}.`)),null!=i&&Vt(Jt(s),(()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i};let h=Je.runKernel("AvgPool",l,u);return h=yn(h,r.dtype),o?In(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const An=nn({clone_:function(t){const e={x:tn(t,"x","clone","string_or_numeric")};return Je.runKernel("Identity",e)}});const Cn=nn({concat_:function(t,e=0){Vt(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=en(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return An(n[0]);const s=n,i={axis:e};return Je.runKernel("Concat",s,i)}});const zn=nn({matMul_:function(t,e,n=!1,s=!1){let i=tn(t,"a","matMul"),r=tn(e,"b","matMul");[i,r]=Ke(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return Je.runKernel("BatchMatMul",a,o)}});const Dn=nn({mul_:function(t,e){let n=tn(t,"a","mul"),s=tn(e,"b","mul");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Multiply",i)}});const $n=nn({sigmoid_:function(t){const e={x:tn(t,"x","sigmoid")};return Je.runKernel("Sigmoid",e)}});const Tn=nn({slice_:function(t,e,n){const s=tn(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Je.runKernel("Slice",i,r)}});const En=nn({tanh_:function(t){const e={x:tn(t,"x","tanh")};return Je.runKernel("Tanh",e)}});const Fn=nn({batchToSpaceND_:function(t,e,n){const s=tn(t,"x","batchToSpaceND"),i=e.reduce(((t,e)=>t*e));Vt(s.rank>=1+e.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`)),Vt(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),Vt(s.shape[0]%i==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${i}`));const r={x:s},a={blockShape:e,crops:n};return Je.runKernel("BatchToSpaceND",r,a)}});const _n=nn({batchNorm_:function(t,e,n,s,i,r){null==r&&(r=.001);const a=tn(t,"x","batchNorm"),o=tn(e,"mean","batchNorm"),l=tn(n,"variance","batchNorm");let u,h;null!=i&&(u=tn(i,"scale","batchNorm")),null!=s&&(h=tn(s,"offset","batchNorm")),Vt(o.rank===l.rank,(()=>"Batch normalization gradient requires mean and variance to have equal ranks.")),Vt(null==h||o.rank===h.rank,(()=>"Batch normalization gradient requires mean and offset to have equal ranks.")),Vt(null==u||o.rank===u.rank,(()=>"Batch normalization gradient requires mean and scale to have equal ranks."));const c={x:function(t){let e;return e=0===t.rank||1===t.rank?In(t,[1,1,1,t.size]):2===t.rank?In(t,[1,1,t.shape[0],t.shape[1]]):3===t.rank?In(t,[1,t.shape[0],t.shape[1],t.shape[2]]):t,e}(a),scale:u,offset:h,mean:o,variance:l},p={varianceEpsilon:r},d=Je.runKernel("FusedBatchNorm",c,p);return In(d,a.shape)}});const Ln=nn({broadcastTo_:function(t,e){let n=tn(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=In(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return An(n);const a={x:n},o={reps:r};return Je.runKernel("Tile",a,o)}});const Rn=nn({ceil_:function(t){const e={x:tn(t,"x","ceil")};return Je.runKernel("Ceil",e)}});const Mn=nn({clipByValue_:function(t,e,n){const s=tn(t,"x","clipByValue");Vt(e<=n,(()=>`Error in clip: min (${e}) must be less than or equal to max (${n}).`));const i={x:s},r={clipValueMin:e,clipValueMax:n};return Je.runKernel("ClipByValue",i,r)}});const On=nn({complex_:function(t,e){const n=tn(t,"real","complex"),s=tn(e,"imag","complex");!function(t,e,n=""){Vt(Ht(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return Je.runKernel("Complex",i)}});const Bn=nn({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=tn(t,"x","conv2d"),l=tn(e,"filter","conv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=In(o,[1,o.shape[0],o.shape[1],o.shape[2]])),Vt(4===u.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${u.rank}.`)),Vt(4===l.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${l.rank}.`)),null!=a&&Vt(Jt(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const c="NHWC"===i?u.shape[3]:u.shape[1];Vt(c===l.shape[2],(()=>`Error in conv2d: depth of input (${c}) must match input depth for filter ${l.shape[2]}.`)),Vt(Sn(n,r),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`));const p={x:u,filter:l},d={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},f=Je.runKernel("Conv2D",p,d);return h?In(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Pn=nn({conv1d_:function(t,e,n,s,i="NWC",r=1,a){const o=tn(t,"x","conv1d"),l=tn(e,"filter","conv1d");let u=o,h=!1;2===o.rank&&(h=!0,u=In(o,[1,o.shape[0],o.shape[1]])),Vt(3===u.rank,(()=>`Error in conv1d: input must be rank 3, but got rank ${u.rank}.`)),Vt(3===l.rank,(()=>`Error in conv1d: filter must be rank 3, but got rank ${l.rank}.`)),null!=a&&Vt(Jt(s),(()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`)),Vt(u.shape[2]===l.shape[1],(()=>`Error in conv1d: depth of input (${u.shape[2]}) must match input depth for filter ${l.shape[1]}.`)),Vt(Sn(n,r),(()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${r}'`)),Vt("NWC"===i,(()=>`Error in conv1d: got dataFormat of ${i} but only NWC is currently supported.`));const c=In(l,[1,l.shape[0],l.shape[1],l.shape[2]]),p=In(u,[u.shape[0],1,u.shape[1],u.shape[2]]),d=Bn(p,c,[1,n],s,"NHWC",[1,r],a);return In(d,h?[d.shape[2],d.shape[3]]:[d.shape[0],d.shape[2],d.shape[3]])}});const Wn=nn({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){Vt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let o=t,l=e,u=!1;3===e.rank&&(u=!0,l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),Vt(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),Vt(4===l.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${l.rank}`)),Vt(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const h="NHWC"===r?o[3]:o[1],c="NHWC"===r?l.shape[3]:l.shape[1];Vt(h===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${h}) must match input depth for filter ${n.shape[2]}.`)),Vt(c===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${c}) must match output depth for filter ${n.shape[3]}.`)),null!=a&&Vt(Jt(i),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const p={dy:l,filter:n},d={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},f=Je.runKernel("Conv2DBackpropInput",p,d);return u?In(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Un=nn({conv2dTranspose_:function(t,e,n,s,i,r){const a=tn(t,"x","conv2dTranspose"),o=tn(e,"filter","conv2dTranspose");return Wn(n,a,o,s,i,"NHWC",r)}});const Kn=nn({conv3DBackpropInput_:function(t,e,n,s,i){Vt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=In(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];Vt(5===r.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${r.length}.`)),Vt(5===a.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${a.rank}`)),Vt(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),Vt(l===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`)),Vt(u===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`));const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=Je.runKernel("Conv3DBackpropInputV2",h,c);return o?In(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}});const jn=nn({cos_:function(t){const e={x:tn(t,"x","cos")};return Je.runKernel("Cos",e)}});const Vn=nn({cosh_:function(t){const e={x:tn(t,"x","cosh")};return Je.runKernel("Cosh",e)}});const qn=nn({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:tn(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Je.runKernel("Cumsum",i,r)}});const Gn=nn({depthToSpace_:function(t,e,n="NHWC"){const s=tn(t,"x","depthToSpace"),i="NHWC"===n?s.shape[1]:s.shape[2],r="NHWC"===n?s.shape[2]:s.shape[3],a="NHWC"===n?s.shape[3]:s.shape[1];Vt(i*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${i} and ${e}  for depthToSpace with input shape\n    ${s.shape}`)),Vt(r*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${e} for depthToSpace with input shape\n        ${s.shape}`)),Vt(a%(e*e)==0,(()=>`Dimension size must be evenly divisible by ${e*e} but is ${a} for depthToSpace with input shape ${s.shape}`));const o={x:s},l={blockSize:e,dataFormat:n};return Je.runKernel("DepthToSpace",o,l)}});const Hn=nn({depthwiseConv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=tn(t,"x","depthwiseConv2d"),l=tn(e,"filter","depthwiseConv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=In(o,[1,o.shape[0],o.shape[1],o.shape[2]])),Vt(4===u.rank,(()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${u.rank}.`)),Vt(4===l.rank,(()=>`Error in depthwiseConv2d: filter must be rank 4, but got rank ${l.rank}.`)),Vt(u.shape[3]===l.shape[2],(()=>`Error in depthwiseConv2d: number of input channels (${u.shape[3]}) must match the inChannels dimension in filter ${l.shape[2]}.`)),null!=a&&Vt(Jt(s),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const c={x:u,filter:l},p={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},d=Je.runKernel("DepthwiseConv2dNative",c,p);return h?In(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});const Jn=nn({dilation2d_:function(t,e,n,s,i=[1,1],r="NHWC"){const a=tn(t,"x","dilation2d"),o=tn(e,"filter","dilation2d");Vt(3===a.rank||4===a.rank,(()=>`Error in dilation2d: input must be rank 3 or 4, but got rank ${a.rank}.`)),Vt(3===o.rank,(()=>`Error in dilation2d: filter must be rank 3, but got rank ${o.rank}.`)),Vt("NHWC"===r,(()=>`Error in dilation2d: Only NHWC is currently supported, but got dataFormat of ${r}`));let l=a,u=!1;3===a.rank&&(l=In(a,[1,a.shape[0],a.shape[1],a.shape[2]]),u=!0);const h={x:l,filter:o},c={strides:n,pad:s,dilations:i},p=Je.runKernel("Dilation2D",h,c);return u?In(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Zn=nn({floorDiv_:function(t,e){let n=tn(t,"a","floorDiv"),s=tn(e,"b","floorDiv");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("FloorDiv",i)}});const Yn=nn({div_:function(t,e){let n=tn(t,"a","div"),s=tn(e,"b","div");if([n,s]=Ke(n,s),"int32"===n.dtype&&"int32"===s.dtype)return Zn(n,s);const i={a:n,b:s};return Je.runKernel("RealDiv",i,{})}});function Xn(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function Qn(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const ts=nn({equal_:function(t,e){let n=tn(t,"a","equal","string_or_numeric"),s=tn(e,"b","equal","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("Equal",i)}});const es=nn({where_:function(t,e,n){const s=tn(e,"a","where"),i=tn(n,"b","where"),r=tn(t,"condition","where","bool"),a=Qn(Qn(r.shape,s.shape),i.shape),o={condition:Ln(r,a),t:Ln(s,a),e:Ln(i,a)};return Je.runKernel("Select",o)}});const ns=nn({zerosLike_:function(t){const e={x:tn(t,"x","zerosLike")};return Je.runKernel("ZerosLike",e)}});const ss=nn({divNoNan_:function(t,e){let n=tn(t,"a","div"),s=tn(e,"b","div");[n,s]=Ke(n,s);const i=Yn(n,s),r=ns(i),a=ts(s,r);return es(a,r,i)}});const is=nn({dot_:function(t,e){const n=tn(t,"t1","dot"),s=tn(e,"t2","dot");Vt(!(1!==n.rank&&2!==n.rank||1!==s.rank&&2!==s.rank),(()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${s.rank}.`));const i=1===n.rank?n.size:n.shape[1],r=1===s.rank?s.size:s.shape[0];if(Vt(i===r,(()=>`Error in dot: inner dimensions of inputs must match, but got ${i} and ${r}.`)),1===n.rank&&1===s.rank){const t=In(n,[1,-1]),e=In(s,[-1,1]),i=zn(t,e);return In(i,[])}if(1===n.rank&&2===s.rank){const t=In(n,[1,-1]),e=In(s,[s.shape[0],s.shape[1]]),i=zn(t,e);return In(i,[i.size])}if(2===n.rank&&1===s.rank){const t=In(s,[-1,1]),e=zn(n,t);return In(e,[e.size])}{const t=In(s,[s.shape[0],s.shape[1]]);return zn(n,t)}}});const rs=nn({elu_:function(t){const e={x:tn(t,"x","elu")};return Je.runKernel("Elu",e)}});const as=nn({erf_:function(t){let e=tn(t,"x","erf");Vt("int32"===e.dtype||"float32"===e.dtype,(()=>"Input dtype must be `int32` or `float32`.")),"int32"===e.dtype&&(e=yn(e,"float32"));const n={x:e};return Je.runKernel("Erf",n)}});const os=nn({exp_:function(t){const e={x:tn(t,"x","exp")};return Je.runKernel("Exp",e)}});const ls=nn({expandDims_:function(t,e=0){const n=tn(t,"x","expandDims","string_or_numeric");Vt(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},i={dim:e};return Je.runKernel("ExpandDims",s,i)}});const us=nn({expm1_:function(t){const e={x:tn(t,"x","expm1")};return Je.runKernel("Expm1",e)}});const hs=nn({tile_:function(t,e){const n=tn(t,"x","tile","string_or_numeric");Vt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const s={x:n},i={reps:e};return Je.runKernel("Tile",s,i)}});const cs=nn({floor_:function(t){const e={x:tn(t,"x","floor")};return Je.runKernel("Floor",e)}});const ps=nn({gather_:function(t,e,n=0,s=0){const i={x:tn(t,"x","gather"),indices:tn(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Je.runKernel("GatherV2",i,r)}});const ds=nn({greater_:function(t,e){let n=tn(t,"a","greater","string_or_numeric"),s=tn(e,"b","greater","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("Greater",i)}});const fs=nn({greaterEqual_:function(t,e){let n=tn(t,"a","greaterEqual","string_or_numeric"),s=tn(e,"b","greaterEqual","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("GreaterEqual",i)}});const gs=nn({imag_:function(t){const e={input:tn(t,"input","imag")};return Je.runKernel("Imag",e)}});const ms=nn({isFinite_:function(t){const e={x:tn(t,"x","isFinite")};return Je.runKernel("IsFinite",e)}});const ys=nn({isInf_:function(t){const e={x:tn(t,"x","isInf")};return Je.runKernel("IsInf",e)}});const bs=nn({isNaN_:function(t){const e={x:tn(t,"x","isNaN")};return Je.runKernel("IsNan",e)}});const ws=nn({leakyRelu_:function(t,e=.2){const n={x:tn(t,"x","leakyRelu")},s={alpha:e};return Je.runKernel("LeakyRelu",n,s)}});const ks=nn({less_:function(t,e){let n=tn(t,"a","less","string_or_numeric"),s=tn(e,"b","less","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("Less",i)}});const xs=nn({lessEqual_:function(t,e){let n=tn(t,"a","lessEqual","string_or_numeric"),s=tn(e,"b","lessEqual","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("LessEqual",i)}});const vs=nn({localResponseNormalization_:function(t,e=5,n=1,s=1,i=.5){const r=tn(t,"x","localResponseNormalization");Vt(4===r.rank||3===r.rank,(()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${r.rank}.`)),Vt(Jt(e),(()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${e}.`));let a=r,o=!1;3===r.rank&&(o=!0,a=In(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={x:a},u={depthRadius:e,bias:n,alpha:s,beta:i},h=Je.runKernel("LRN",l,u);return o?In(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Ss=nn({log_:function(t){const e={x:tn(t,"x","log")};return Je.runKernel("Log",e)}});const Is=nn({log1p_:function(t){const e={x:tn(t,"x","log1p")};return Je.runKernel("Log1p",e)}});function Ns(t){return Je.customGrad(t)}const As=nn({neg_:function(t){const e={x:tn(t,"x","neg")};return Je.runKernel("Neg",e)}});const Cs=nn({softplus_:function(t){const e={x:tn(t,"x","softplus")};return Je.runKernel("Softplus",e)}});const zs=nn({logSigmoid_:function(t){const e=tn(t,"x","logSigmoid");return Ns((t=>({value:As(Cs(As(t))),gradFunc:e=>Dn(e,$n(As(t)))})))(e)}});const Ds=nn({max_:function(t,e=null,n=!1){const s={x:tn(t,"x","max")},i={reductionIndices:e,keepDims:n};return Je.runKernel("Max",s,i)}});const $s=nn({sub_:function(t,e){let n=tn(t,"a","sub"),s=tn(e,"b","sub");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Sub",i)}});const Ts=nn({sum_:function(t,e=null,n=!1){let s=tn(t,"x","sum");"bool"===s.dtype&&(s=yn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Je.runKernel("Sum",i,r)}});const Es=nn({logSoftmax_:function(t,e=-1){const n=tn(t,"logits","logSoftmax");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${e}`);return Ns(((t,n)=>{const s=Ds(t,e,!0),i=$s(t,s),r=$s(yn(i,"float32"),Ss(Ts(os(i),e,!0)));n([r]);return{value:r,gradFunc:(t,n)=>{const[s]=n,i=os(s);return $s(t,Dn(Ts(t,e,!0),i))}}}))(n)}});function Fs(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function _s(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const Ls=nn({logSumExp_:function(t,e=null,n=!1){const s=tn(t,"x","logSumExp"),i=Yt(e,s.shape),r=Ds(s,i,!0),a=$s(s,r),o=os(a),l=Ts(o,i),u=Ss(l),h=on(In(r,u.shape),u);if(n){const t=Fs(h.shape,i);return In(h,t)}return h}});const Rs=nn({logicalAnd_:function(t,e){const n=tn(t,"a","logicalAnd","bool"),s=tn(e,"b","logicalAnd","bool");Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("LogicalAnd",i)}});const Ms=nn({logicalNot_:function(t){const e={x:tn(t,"x","logicalNot","bool")};return Je.runKernel("LogicalNot",e)}});const Os=nn({logicalOr_:function(t,e){const n=tn(t,"a","logicalOr","bool"),s=tn(e,"b","logicalOr","bool");Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("LogicalOr",i)}});const Bs=nn({logicalXor_:function(t,e){const n=tn(t,"a","logicalXor","bool"),s=tn(e,"b","logicalXor","bool");return Qn(n.shape,s.shape),Rs(Os(t,e),Ms(Rs(t,e)))}});const Ps=nn({maxPool_:function(t,e,n,s,i){const r=tn(t,"x","maxPool");let a=r,o=!1;3===r.rank&&(o=!0,a=In(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===a.rank,(()=>`Error in maxPool: input must be rank 4 but got rank ${a.rank}.`)),Vt(Sn(n,1),(()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`)),null!=i&&Vt(Jt(s),(()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i},h=Je.runKernel("MaxPool",l,u);return o?In(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Ws=nn({maximum_:function(t,e){let n=tn(t,"a","maximum"),s=tn(e,"b","maximum");[n,s]=Ke(n,s),"bool"===n.dtype&&(n=yn(n,"int32"),s=yn(s,"int32")),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("Maximum",i)}});const Us=nn({mean_:function(t,e=null,n=!1){const s={x:tn(t,"x","mean")},i={axis:e,keepDims:n};return Je.runKernel("Mean",s,i)}});function Ks(t,e="float32"){if("complex64"===e){const e=Ks(t,"float32"),n=Ks(t,"float32");return On(e,n)}const n=oe(Gt(t),e);return Je.makeTensor(n,t,e)}function js(t,e="float32"){if("complex64"===e){const e=js(t,"float32"),n=Ks(t,"float32");return On(e,n)}const n=ae(Gt(t),e);return Je.makeTensor(n,t,e)}const Vs=nn({min_:function(t,e=null,n=!1){const s={x:tn(t,"x","min")},i={axis:e,keepDims:n};return Je.runKernel("Min",s,i)}});const qs=nn({minimum_:function(t,e){let n=tn(t,"a","minimum"),s=tn(e,"b","minimum");[n,s]=Ke(n,s),"bool"===n.dtype&&(n=yn(n,"int32"),s=yn(s,"int32")),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("Minimum",i)}});const Gs=nn({mirrorPad_:function(t,e,n){Vt("reflect"===n||"symmetric"===n,(()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`));const s=tn(t,"x","mirrorPad");if(0===s.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");Vt(e.length===s.rank,(()=>`Padding doesn't match input. Must be ${s.rank}. Got ${e.length}.`));const i="reflect"===n?1:0;for(let t=0;t<s.rank;t++)Vt(2===e[t].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),Vt(e[t][0]>=0&&e[t][0]<=s.shape[t]-i&&e[t][1]>=0&&e[t][1]<=s.shape[t]-i,(()=>`Padding in dimension ${t} cannot be greater than or equal to ${s.shape[t]-i} or less than 0 for input of shape ${s.shape}`));const r={paddings:e,mode:n},a={x:s};return Je.runKernel("MirrorPad",a,r)}});const Hs=nn({mod_:function(t,e){let n=tn(t,"a","mod"),s=tn(e,"b","mod");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Mod",i)}});const Js=nn({square_:function(t){const e=tn(t,"x","square");return Je.runKernel("Square",{x:e},{})}});const Zs=nn({notEqual_:function(t,e){let n=tn(t,"a","notEqual","string_or_numeric"),s=tn(e,"b","notEqual","string_or_numeric");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("NotEqual",i)}});const Ys=nn({oneHot_:function(t,e,n=1,s=0){if(e<2)throw new Error(`Error in oneHot: depth must be >=2, but it is ${e}`);const i={indices:tn(t,"indices","oneHot","int32")},r={depth:e,onValue:n,offValue:s};return Je.runKernel("OneHot",i,r)}});const Xs=nn({onesLike_:function(t){const e={x:tn(t,"x","onesLike")};return Je.runKernel("OnesLike",e)}});const Qs=nn({pad_:function(t,e,n=0){const s=tn(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Je.runKernel("PadV2",r,i)}});const ti=nn({spaceToBatchND_:function(t,e,n){const s=tn(t,"x","spaceToBatchND");Vt(s.rank>=1+e.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`)),Vt(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),Vt(s.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const i={x:s},r={blockShape:e,paddings:n};return Je.runKernel("SpaceToBatchND",i,r)}});const ei=nn({pool_:function(t,e,n,s,i,r){null==i&&(i=[1,1]),null==r&&(r=1),0===s&&(s="valid");const a=tn(t,"x","maxPool");let o=a,l=!1;3===a.rank&&(l=!0,o=In(a,[1,a.shape[0],a.shape[1],a.shape[2]])),Vt(Sn(r,i),(()=>`Error in pool: Either strides or dilations must be 1. Got strides ${r} and dilations '${i}'`));const u=bn(o.shape,e,r,i,s),h=[u.dilationHeight,u.dilationWidth];let c;c="same"===s?function(t,e){const n=t.map(((t,n)=>t+(t-1)*(e[n]-1))).map((t=>t-1)),s=n.map((t=>Math.floor(t/2))),i=n.map(((t,e)=>t-s[e]));return n.map(((t,e)=>[s[e],i[e]]))}([u.filterHeight,u.filterWidth],h):[[0,0],[0,0]];const p=1===h[0]&&1===h[1],[d,f]=function(t,e,n){const s=n.map((t=>t[0])),i=n.map((t=>t[1])),r=t.concat(s,i),a=e.map(((t,e)=>(t-r[e]%t)%t)),o=i.map(((t,e)=>t+a[e])),l=e.map(((t,e)=>[s[e],o[e]])),u=e.map(((t,e)=>[0,a[e]]));return[l,u]}([u.inHeight,u.inWidth],h,c),g=p?s:"valid",m=p?o:ti(o,h,d),y=("avg"===n?()=>Nn(m,e,r,g):()=>Ps(m,e,r,g))(),b=p?y:Fn(y,h,f);return l?In(b,[b.shape[1],b.shape[2],b.shape[3]]):b}});const ni=nn({pow_:function(t,e){let n=tn(t,"base","pow"),s=tn(e,"exp","pow");[n,s]=Ke(n,s);const i={a:n,b:s};return Je.runKernel("Pow",i)}});const si=nn({prelu_:function(t,e){const n={x:tn(t,"x","prelu"),alpha:tn(e,"alpha","prelu")};return Je.runKernel("Prelu",n)}});const ii=nn({prod_:function(t,e=null,n=!1){let s=tn(t,"x","prod");"bool"===s.dtype&&(s=yn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Je.runKernel("Prod",i,r)}});const ri=nn({real_:function(t){const e={input:tn(t,"input","real")};return Je.runKernel("Real",e)}});const ai=nn({reciprocal_:function(t){const e={x:tn(t,"x","reciprocal")};return Je.runKernel("Reciprocal",e)}});const oi=nn({relu_:function(t){const e={x:tn(t,"x","relu")};return Je.runKernel("Relu",e)}});const li=nn({relu6_:function(t){const e={x:tn(t,"x","relu6")};return Je.runKernel("Relu6",e)}});const ui=nn({reverse_:function(t,e){const n={x:tn(t,"x","reverse")},s={dims:e};return Je.runKernel("Reverse",n,s)}});const hi=nn({round_:function(t){const e={x:tn(t,"x","round")};return Je.runKernel("Round",e)}});const ci=nn({rsqrt_:function(t){const e={x:tn(t,"x","rsqrt")};return Je.runKernel("Rsqrt",e)}});function pi(t,e,n,s){if(null==s&&(s=ee(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!Xt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{Vt(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=Gt(e),s=Gt(n);Vt(t===s,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`));for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==Gt(e.slice(t));Vt(n[t]===e[t]||!i,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return Xt(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?ve(t,s):qt(t,[],!0),Je.makeTensor(t,e,s)}function di(t,e){if((Xt(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&Xt(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return pi(t,[],[],e)}const fi=nn({selu_:function(t){const e={x:tn(t,"x","selu")};return Je.runKernel("Selu",e)}});const gi=nn({separableConv2d_:function(t,e,n,s,i,r=[1,1],a="NHWC"){const o=tn(t,"x","separableConv2d"),l=tn(e,"depthwiseFilter","separableConv2d"),u=tn(n,"pointwiseFilter","separableConv2d");let h=o,c=!1;if(3===o.rank&&(c=!0,h=In(o,[1,o.shape[0],o.shape[1],o.shape[2]])),"NCHW"===a)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");Vt(4===h.rank,(()=>`Error in separableConv2d: input must be rank 4, but got rank ${h.rank}.`)),Vt(4===l.rank,(()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${l.rank}.`)),Vt(4===u.rank,(()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${l.rank}.`)),Vt(1===u.shape[0],(()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${u.shape[0]}.`)),Vt(1===u.shape[1],(()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${u.shape[1]}.`));const p=l.shape[2],d=l.shape[3];Vt(u.shape[2]===p*d,(()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${p*d}, but got ${u.shape[2]}.`));const f=Hn(h,l,s,i,a,r),g=Bn(f,u,1,"valid",a);return c?In(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const mi=nn({sign_:function(t){const e={x:tn(t,"x","sign")};return Je.runKernel("Sign",e)}});const yi=nn({sin_:function(t){const e={x:tn(t,"x","sin")};return Je.runKernel("Sin",e)}});const bi=nn({sinh_:function(t){const e={x:tn(t,"x","sinh")};return Je.runKernel("Sinh",e)}});const wi=nn({softmax_:function(t,e=-1){const n=tn(t,"logits","softmax","float32");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${e}`);const s={logits:n},i={dim:e};return Je.runKernel("Softmax",s,i)}});const ki=nn({fft_:function(t){Vt("complex64"===t.dtype,(()=>`The dtype for tf.spectral.fft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Je.runKernel("FFT",e)}});const xi=nn({ifft_:function(t){Vt("complex64"===t.dtype,(()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Je.runKernel("IFFT",e)}});const vi=nn({irfft_:function(t){const e=t.shape[t.shape.length-1],n=t.size/e;let s;if(e<=2){const i=In(t,[n,e]);s=xi(i)}else{const i=[n,2*(e-1)],r=In(ri(t),[n,e]),a=In(gs(t),[n,e]),o=ui(Tn(r,[0,1],[n,e-2]),1),l=Dn(ui(Tn(a,[0,1],[n,e-2]),1),di(-1)),u=Cn([r,o],1),h=Cn([a,l],1),c=In(On(u,h),[i[0],i[1]]);s=xi(c)}if(s=ri(s),3===t.rank&&0!==t.shape[0]){const e=s,n=t.shape[0];s=In(s,[n,s.shape[0]/n,s.shape[1]]),e.dispose()}return s}});const Si=nn({split_:function(t,e,n=0){const s={x:tn(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Je.runKernel("SplitV",s,i)}});const Ii=nn({rfft_:function(t,e){Vt("float32"===t.dtype,(()=>`The dtype for rfft() must be real value but got ${t.dtype}`));let n=t.shape[t.shape.length-1];const s=t.size/n;let i;if(null!=e&&e<n){const s=t.shape.map((t=>0)),r=t.shape.map((t=>t));r[t.shape.length-1]=e,i=Tn(t,s,r),n=e}else if(null!=e&&e>n){const s=t.shape.map((t=>t));s[t.shape.length-1]=e-n,i=Cn([t,Ks(s)],t.shape.length-1),n=e}else i=t;const r=ns(i),a=In(On(i,r),[s,n]),o=ki(a),l=Math.floor(n/2)+1,u=ri(o),h=gs(o),c=Si(u,[l,n-l],u.shape.length-1),p=Si(h,[l,n-l],h.shape.length-1),d=i.shape.slice();return d[i.shape.length-1]=l,In(On(c[0],p[0]),d)}});const Ni=nn({sqrt_:function(t){const e={x:tn(t,"x","sqrt")};return Je.runKernel("Sqrt",e)}});const Ai=nn({squaredDifference_:function(t,e){let n=tn(t,"a","squaredDifference"),s=tn(e,"b","squaredDifference");[n,s]=Ke(n,s),Qn(n.shape,s.shape);const i={a:n,b:s};return Je.runKernel("SquaredDifference",i,{})}});const Ci=nn({squeeze_:function(t,e){const n=tn(t,"x","squeeze");return In(n,function(t,e){const n=[],s=[],i=null!=e&&Array.isArray(e)&&0===e.length,r=null==e||i?null:Yt(e,t).sort();let a=0;for(let e=0;e<t.length;++e){if(null!=r){if(r[a]===e&&1!==t[e])throw new Error(`Can't squeeze axis ${e} since its dim '${t[e]}' is not 1`);(null==r[a]||r[a]>e)&&1===t[e]&&(n.push(t[e]),s.push(e)),r[a]<=e&&a++}1!==t[e]&&(n.push(t[e]),s.push(e))}return{newShape:n,keptDims:s}}(n.shape,e).newShape)}});const zi=nn({stack_:function(t,e=0){const n=en(t,"tensors","stack","string_or_numeric");Vt(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&Vt(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,i={axis:e};return Je.runKernel("Pack",s,i)}});const Di=nn({step_:function(t,e=0){const n={x:tn(t,"x","step")},s={alpha:e};return Je.runKernel("Step",n,s)}});const $i=nn({stridedSlice_:function(t,e,n,s,i=0,r=0,a=0,o=0,l=0){const u={x:tn(t,"x","stridedSlice","string_or_numeric")},h={begin:e,end:n,strides:s,beginMask:i,endMask:r,ellipsisMask:a,newAxisMask:o,shrinkAxisMask:l};return Je.runKernel("StridedSlice",u,h)}});const Ti=nn({tan_:function(t){const e={x:tn(t,"x","tan")};return Je.runKernel("Tan",e)}});const Ei=nn({topk_:function(t,e=1,n=!0){const s=tn(t,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const i=s.shape[s.shape.length-1];if(e>i)throw new Error(`'k' passed to topk() must be <= the last dimension (${i}) but got ${e}`);const r={x:s},a={k:e,sorted:n},[o,l]=Je.runKernel("TopK",r,a);return{values:o,indices:l}}});const Fi=nn({unique_:function(t,e=0){const n=tn(t,"x","unique","string_or_numeric");Vt(n.rank>0,(()=>"The input tensor must be at least 1D"));const s={x:n},i={axis:e},[r,a]=Je.runKernel("Unique",s,i);return{values:r,indices:a}}});const _i=nn({unsortedSegmentSum_:function(t,e,n){const s=tn(t,"x","unsortedSegmentSum"),i=tn(e,"segmentIds","unsortedSegmentSum","int32");Vt(Jt(n),(()=>"numSegments must be of dtype int"));const r={x:s,segmentIds:i},a={numSegments:n};return Je.runKernel("UnsortedSegmentSum",r,a)}});const Li=nn({unstack_:function(t,e=0){const n=tn(t,"x","unstack","string_or_numeric");Vt(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},i={axis:e};return Je.runKernel("Unpack",s,i)}});const Ri=nn({transpose_:function(t,e){const n=tn(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),Vt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{Vt(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return Je.runKernel("Transpose",s,i)}});function Mi(t,e,n=null){if(0===t.rank)return sn(t);if(1!==t.rank&&null===n)return Mi(In(t,[-1]),e,n);if(1===t.rank||"number"==typeof n||Array.isArray(n)&&1===n.length){if(1===e)return Ts(sn(t),n);if(e===1/0)return Ds(sn(t),n);if(e===-1/0)return Vs(sn(t),n);if("euclidean"===e||2===e)return Ni(Ts(ni(sn(t),di(2,"int32")),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}if(Array.isArray(n)&&2===n.length){if(1===e)return Ds(Ts(sn(t),n[0]),n[1]-1);if(e===1/0)return Ds(Ts(sn(t),n[1]),n[0]);if(e===-1/0)return Vs(Ts(sn(t),n[1]),n[0]);if("fro"===e||"euclidean"===e)return Ni(Ts(Js(t),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}throw new Error(`Error in norm: invalid axis: ${n}`)}const Oi=nn({norm_:function(t,e="euclidean",n=null,s=!1){const i=Mi(t=tn(t,"x","norm"),e,n);let r=i.shape;if(s){const e=Yt(n,t.shape);r=Fs(i.shape,e)}return In(i,r)}});const Bi=nn({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=In(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]])),Vt(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),Vt(4===l.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${l.shape}.`)),Vt(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];Vt(u===n[2],(()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`)),Vt(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=a&&Vt(Jt(i),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return Je.runKernel("Conv2DBackpropFilter",c,p)}});const Pi=nn({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=In(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Je.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Wi=nn({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=In(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Je.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?In(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Ui=nn({resizeBilinear_:function(t,e,n=!1,s=!1){const i=tn(t,"images","resizeBilinear");Vt(3===i.rank||4===i.rank,(()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${i.rank}.`)),Vt(2===e.length,(()=>`Error in resizeBilinear: new shape must 2D, but got shape ${e}.`)),Vt(!1===s||!1===n,(()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false."));let r=i,a=!1;3===i.rank&&(a=!0,r=In(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Je.runKernel("ResizeBilinear",o,l);return a?In(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});const Ki=nn({resizeNearestNeighbor_:function(t,e,n=!1,s=!1){const i=tn(t,"images","resizeNearestNeighbor");Vt(3===i.rank||4===i.rank,(()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${i.rank}.`)),Vt(2===e.length,(()=>`Error in resizeNearestNeighbor: new shape must 2D, but got shape ${e}.`)),Vt("float32"===i.dtype||"int32"===i.dtype,(()=>"`images` must have `int32` or `float32` as dtype")),Vt(!1===s||!1===n,(()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false."));let r=i,a=!1;3===i.rank&&(a=!0,r=In(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Je.runKernel("ResizeNearestNeighbor",o,l);return a?In(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});Le().prototype.abs=function(){return this.throwIfDisposed(),sn(this)},Le().prototype.acos=function(){return this.throwIfDisposed(),rn(this)},Le().prototype.acosh=function(){return this.throwIfDisposed(),an(this)},Le().prototype.add=function(t){return this.throwIfDisposed(),on(this,t)},Le().prototype.all=function(t,e){return this.throwIfDisposed(),ln(this,t,e)},Le().prototype.any=function(t,e){return this.throwIfDisposed(),un(this,t,e)},Le().prototype.argMax=function(t){return this.throwIfDisposed(),hn(this,t)},Le().prototype.argMin=function(t){return this.throwIfDisposed(),cn(this,t)},Le().prototype.asScalar=function(){return this.throwIfDisposed(),Vt(1===this.size,(()=>"The array must have only 1 element.")),In(this,[])},Le().prototype.asType=function(t){return this.throwIfDisposed(),yn(this,t)},Le().prototype.as1D=function(){return this.throwIfDisposed(),In(this,[this.size])},Le().prototype.as2D=function(t,e){return this.throwIfDisposed(),In(this,[t,e])},Le().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),In(this,[t,e,n])},Le().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),In(this,[t,e,n,s])},Le().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),In(this,[t,e,n,s,i])},Le().prototype.asin=function(){return this.throwIfDisposed(),pn(this)},Le().prototype.asinh=function(){return this.throwIfDisposed(),dn(this)},Le().prototype.atan=function(){return this.throwIfDisposed(),fn(this)},Le().prototype.atan2=function(t){return this.throwIfDisposed(),gn(this,t)},Le().prototype.atanh=function(){return this.throwIfDisposed(),mn(this)},Le().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),Nn(this,t,e,n,s)},Le().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),Fn(this,t,e)},Le().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),_n(this,t,e,n,s,i)},Le().prototype.broadcastTo=function(t){return this.throwIfDisposed(),Ln(this,t)},Le().prototype.cast=function(t){return this.throwIfDisposed(),yn(this,t)},Le().prototype.ceil=function(){return this.throwIfDisposed(),Rn(this)},Le().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),Mn(this,t,e)},Le().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof _e&&(t=[t]),Cn([this,...t],e)},Le().prototype.conv1d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Pn(this,t,e,n,s,i,r)},Le().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),Un(this,t,e,n,s,i)},Le().prototype.conv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Bn(this,t,e,n,s,i,r)},Le().prototype.cos=function(){return this.throwIfDisposed(),jn(this)},Le().prototype.cosh=function(){return this.throwIfDisposed(),Vn(this)},Le().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),qn(this,t,e,n)},Le().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),Gn(this,t,e)},Le().prototype.depthwiseConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Hn(this,t,e,n,s,i,r)},Le().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),Jn(this,t,e,n,s,i)},Le().prototype.divNoNan=function(t){return this.throwIfDisposed(),ss(this,t)},Le().prototype.div=function(t){return this.throwIfDisposed(),Yn(this,t)},Le().prototype.dot=function(t){return this.throwIfDisposed(),is(this,t)},Le().prototype.elu=function(){return this.throwIfDisposed(),rs(this)},Le().prototype.equal=function(t){return this.throwIfDisposed(),ts(this,t)},Le().prototype.erf=function(){return this.throwIfDisposed(),as(this)},Le().prototype.exp=function(){return this.throwIfDisposed(),os(this)},Le().prototype.expandDims=function(t){return this.throwIfDisposed(),ls(this,t)},Le().prototype.expm1=function(){return this.throwIfDisposed(),us(this)},Le().prototype.fft=function(){return this.throwIfDisposed(),ki(this)},Le().prototype.flatten=function(){return this.throwIfDisposed(),In(this,[this.size])},Le().prototype.floor=function(){return this.throwIfDisposed(),cs(this)},Le().prototype.floorDiv=function(t){return this.throwIfDisposed(),Zn(this,t)},Le().prototype.gather=function(t,e){return this.throwIfDisposed(),ps(this,t,e)},Le().prototype.greaterEqual=function(t){return this.throwIfDisposed(),fs(this,t)},Le().prototype.greater=function(t){return this.throwIfDisposed(),ds(this,t)},Le().prototype.ifft=function(){return this.throwIfDisposed(),xi(this)},Le().prototype.irfft=function(){return this.throwIfDisposed(),vi(this)},Le().prototype.isFinite=function(){return this.throwIfDisposed(),ms(this)},Le().prototype.isInf=function(){return this.throwIfDisposed(),ys(this)},Le().prototype.isNaN=function(){return this.throwIfDisposed(),bs(this)},Le().prototype.leakyRelu=function(t){return this.throwIfDisposed(),ws(this,t)},Le().prototype.lessEqual=function(t){return this.throwIfDisposed(),xs(this,t)},Le().prototype.less=function(t){return this.throwIfDisposed(),ks(this,t)},Le().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),vs(this,t,e,n,s)},Le().prototype.logSigmoid=function(){return this.throwIfDisposed(),zs(this)},Le().prototype.logSoftmax=function(t){return this.throwIfDisposed(),Es(this,t)},Le().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),Ls(this,t,e)},Le().prototype.log=function(){return this.throwIfDisposed(),Ss(this)},Le().prototype.log1p=function(){return this.throwIfDisposed(),Is(this)},Le().prototype.logicalAnd=function(t){return this.throwIfDisposed(),Rs(this,t)},Le().prototype.logicalNot=function(){return this.throwIfDisposed(),Ms(this)},Le().prototype.logicalOr=function(t){return this.throwIfDisposed(),Os(this,t)},Le().prototype.logicalXor=function(t){return this.throwIfDisposed(),Bs(this,t)},Le().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),zn(this,t,e,n)},Le().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),Ps(this,t,e,n,s)},Le().prototype.max=function(t,e){return this.throwIfDisposed(),Ds(this,t,e)},Le().prototype.maximum=function(t){return this.throwIfDisposed(),Ws(this,t)},Le().prototype.mean=function(t,e){return this.throwIfDisposed(),Us(this,t,e)},Le().prototype.min=function(t,e){return this.throwIfDisposed(),Vs(this,t,e)},Le().prototype.minimum=function(t){return this.throwIfDisposed(),qs(this,t)},Le().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),Gs(this,t,e)},Le().prototype.mod=function(t){return this.throwIfDisposed(),Hs(this,t)},Le().prototype.mul=function(t){return this.throwIfDisposed(),Dn(this,t)},Le().prototype.neg=function(){return this.throwIfDisposed(),As(this)},Le().prototype.norm=function(t,e,n){return this.throwIfDisposed(),Oi(this,t,e,n)},Le().prototype.notEqual=function(t){return this.throwIfDisposed(),Zs(this,t)},Le().prototype.oneHot=function(t,e=1,n=0){return this.throwIfDisposed(),Ys(this,t,e,n)},Le().prototype.onesLike=function(){return this.throwIfDisposed(),Xs(this)},Le().prototype.pad=function(t,e){return this.throwIfDisposed(),Qs(this,t,e)},Le().prototype.pool=function(t,e,n,s,i){return this.throwIfDisposed(),ei(this,t,e,n,s,i)},Le().prototype.pow=function(t){return this.throwIfDisposed(),ni(this,t)},Le().prototype.prelu=function(t){return this.throwIfDisposed(),si(this,t)},Le().prototype.prod=function(t,e){return this.throwIfDisposed(),ii(this,t,e)},Le().prototype.reciprocal=function(){return this.throwIfDisposed(),ai(this)},Le().prototype.relu=function(){return this.throwIfDisposed(),oi(this)},Le().prototype.relu6=function(){return this.throwIfDisposed(),li(this)},Le().prototype.reshapeAs=function(t){return this.throwIfDisposed(),In(this,t.shape)},Le().prototype.reshape=function(t){return this.throwIfDisposed(),In(this,t)},Le().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),Ui(this,t,e,n)},Le().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),Ki(this,t,e,n)},Le().prototype.reverse=function(t){return this.throwIfDisposed(),ui(this,t)},Le().prototype.rfft=function(){return this.throwIfDisposed(),Ii(this)},Le().prototype.round=function(){return this.throwIfDisposed(),hi(this)},Le().prototype.rsqrt=function(){return this.throwIfDisposed(),ci(this)},Le().prototype.selu=function(){return this.throwIfDisposed(),fi(this)},Le().prototype.separableConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),gi(this,t,e,n,s,i,r)},Le().prototype.sigmoid=function(){return this.throwIfDisposed(),$n(this)},Le().prototype.sign=function(){return this.throwIfDisposed(),mi(this)},Le().prototype.sin=function(){return this.throwIfDisposed(),yi(this)},Le().prototype.sinh=function(){return this.throwIfDisposed(),bi(this)},Le().prototype.slice=function(t,e){return this.throwIfDisposed(),Tn(this,t,e)},Le().prototype.softmax=function(t){return this.throwIfDisposed(),wi(this,t)},Le().prototype.softplus=function(){return this.throwIfDisposed(),Cs(this)},Le().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),ti(this,t,e)},Le().prototype.split=function(t,e){return this.throwIfDisposed(),Si(this,t,e)},Le().prototype.sqrt=function(){return this.throwIfDisposed(),Ni(this)},Le().prototype.square=function(){return this.throwIfDisposed(),Js(this)},Le().prototype.squaredDifference=function(t){return this.throwIfDisposed(),Ai(this,t)},Le().prototype.squeeze=function(t){return this.throwIfDisposed(),Ci(this,t)},Le().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof _e?[this,t]:[this,...t];return zi(n,e)},Le().prototype.step=function(t){return this.throwIfDisposed(),Di(this,t)},Le().prototype.stridedSlice=function(t,e,n,s,i,r,a,o){return this.throwIfDisposed(),$i(this,t,e,n,s,i,r,a,o)},Le().prototype.sub=function(t){return this.throwIfDisposed(),$s(this,t)},Le().prototype.sum=function(t,e){return this.throwIfDisposed(),Ts(this,t,e)},Le().prototype.tan=function(){return this.throwIfDisposed(),Ti(this)},Le().prototype.tanh=function(){return this.throwIfDisposed(),En(this)},Le().prototype.tile=function(t){return this.throwIfDisposed(),hs(this,t)},Le().prototype.toBool=function(){return this.throwIfDisposed(),yn(this,"bool")},Le().prototype.toFloat=function(){return this.throwIfDisposed(),yn(this,"float32")},Le().prototype.toInt=function(){return this.throwIfDisposed(),yn(this,"int32")},Le().prototype.topk=function(t,e){return this.throwIfDisposed(),Ei(this,t,e)},Le().prototype.transpose=function(t){return this.throwIfDisposed(),Ri(this,t)},Le().prototype.unique=function(t){return this.throwIfDisposed(),Fi(this,t)},Le().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),_i(this,t,e)},Le().prototype.unstack=function(t){return this.throwIfDisposed(),Li(this,t)},Le().prototype.where=function(t,e){return this.throwIfDisposed(),es(t,this,e)},Le().prototype.zerosLike=function(){return this.throwIfDisposed(),ns(this)};const ji={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,Di(yn(n,"float32"),-1))}}},Vi={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Js(yn(n,"float32")),s=Ni($s(di(1),e));return As(Yn(t,s))}}}},qi={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Ni($s(Js(yn(n,"float32")),1));return Yn(t,e)}}}},Gi={kernelName:"Add",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{let e=t;const s=Xn(n.shape,i);return s.length>0&&(e=Ts(e,s)),In(e,n.shape)},b:()=>{let e=t;const n=Xn(s.shape,i);return n.length>0&&(e=Ts(e,n)),In(e,s.shape)}}}},Hi={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}},Ji={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ns(n)}}},Zi={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ns(n)}}},Yi={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Ni($s(di(1),Js(yn(n,"float32")))))}}},Xi={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Ni(on(di(1),Js(yn(n,"float32"))));return Yn(t,e)}}}},Qi={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{const e=on(Js(n),Js(s));let r=Dn(t,Yn(s,e));const a=Xn(n.shape,i);return a.length>0&&(r=Ts(r,a)),In(r,n.shape)},b:()=>{const e=on(Js(n),Js(s));let r=As(Dn(t,Yn(n,e)));const a=Xn(s.shape,i);return a.length>0&&(r=Ts(r,a)),In(r,s.shape)}}}},tr={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,on(Js(yn(n,"float32")),1))}}},er={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,$s(di(1),Js(yn(n,"float32"))))}}};const nr=nn({avgPool3dGrad_:function(t,e,n,s,i,r){const a=tn(t,"dy","avgPool3dGrad"),o=tn(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=In(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=In(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),Vt(5===l.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${l.rank}.`)),Vt(5===u.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${u.rank}.`)),null!=r&&Vt(Jt(i),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=Je.runKernel("AvgPool3DGrad",c,p);return h?In(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),sr={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>nr(t,s,i,r,a,o)}}};const ir=nn({avgPoolGrad_:function(t,e,n,s,i){const r=tn(t,"dy","avgPoolGrad"),a=tn(e,"input","avgPoolGrad");Vt(a.rank===r.rank,(()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`));let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=In(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=In(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===l.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${l.rank}.`)),Vt(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=Je.runKernel("AvgPoolGrad",h,c);return u?In(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),rr={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>ir(t,s,i,r,a)}}},ar={kernelName:"BatchMatMul",inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>zn(t,i,!1,!1),b:()=>zn(t,s,!0,!1)}:r&&!a?{a:()=>zn(i,t,!1,!0),b:()=>zn(s,t,!1,!1)}:{a:()=>zn(i,t,!0,!0),b:()=>zn(t,s,!0,!0)}:{a:()=>zn(t,i,!1,!0),b:()=>zn(s,t,!0,!1)}}},or={kernelName:"BatchToSpaceND",gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>ti(t,s,i)}}},lr={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>Ts(t,o,!0)}}},ur={kernelName:"Cast",gradFunc:t=>({x:()=>t.clone()})},hr={kernelName:"Ceil",gradFunc:t=>({x:()=>ns(t)})},cr={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>es(Rs(fs(s,i),xs(s,r)),t,ns(t))}}},pr={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:ji.gradFunc},dr={kernelName:"Concat",saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=Yt(i,e[0].shape)[0],a=s.map((t=>t[r]));return Si(t,a,r).map((t=>()=>t))}},fr={kernelName:"Conv2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return Vt(vn(r),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`)),{x:()=>Wn(s.shape,t,i,a,o,l),filter:()=>Bi(s,t,i.shape,a,o,l)}}},gr={kernelName:"Conv2DBackpropInput",inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>Bn(t,i,r,a,o,1,l),filter:()=>Bi(t,s,i.shape,r,a,o,l)}}};const mr=nn({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=In(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=In(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),Vt(5===r.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${r.shape}.`)),Vt(5===a.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${a.shape}.`)),Vt(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),Vt(r.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`)),Vt(a.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return Je.runKernel("Conv3DBackpropFilterV2",o,l)}}),yr={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;Vt(vn(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[a,o]=e;return{x:()=>Kn(a.shape,t,o,i,r),filter:()=>mr(a,t,o.shape,i,r)}}},br={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(As(yi(yn(n,"float32"))),t)}}},wr={kernelName:"Cosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(bi(yn(n,"float32")),t)}}},kr={kernelName:"Cumsum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=qn(t,i,r,!a);return null!=e&&(n=Ri(n,e)),n}}}},xr={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;Vt(vn(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[l,u]=e;return Vt(4===l.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`)),Vt(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`)),Vt(l.shape[3]===u.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),Vt(Sn(i,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`)),null!=a&&Vt(Jt(r),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`)),{x:()=>Wi(l.shape,t,u,i,r,s,a),filter:()=>Pi(l,t,u.shape,i,r,s,a)}}},vr={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Je.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Je.runKernel("Dilation2DBackpropFilter",a,n)}}},Sr={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Je.runKernel("EluGrad",s)}}},Ir={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=Dn(os(As(Js(n))),2/Math.sqrt(Math.PI));return{x:()=>Dn(t,s)}}},Nr={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,n)}}},Ar={kernelName:"ExpandDims",inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>In(t,n.shape)}}},Cr={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,os(n))}}},zr={kernelName:"Floor",gradFunc:t=>({x:()=>ns(t)})},Dr={kernelName:"FloorDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{const e=Yn(t,yn(s,"float32")),r=Xn(n.shape,i);return r.length>0?In(Ts(e,r),n.shape):e},b:()=>{let e=Dn(t,yn(n,"float32"));const r=Xn(s.shape,i);r.length>0&&(e=In(Ts(e,r),s.shape));const a=Js(s);return As(Yn(e,yn(a,"float32")))}}}},$r={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?di(1):o,u=Xn(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=$s(i,r),p=Dn(t,l),d=ci(on(a,di(s))),f=Dn(Dn(Dn(d,d),d),di(-.5));return{x:()=>1===r.rank?In(Dn(Dn(t,hs(In(d,[1,1,1,r.shape[0]]),h)),l),i.shape):In(Dn(Dn(t,d),l),i.shape),mean:()=>{let t=Dn(Dn(d,di(-1)),p);return 1===r.rank&&(t=Ts(t,u)),In(t,r.shape)},variance:()=>{let t=Dn(Dn(f,c),p);return 1===r.rank&&(t=Ts(t,u)),In(t,r.shape)},scale:()=>{const e=Dn(c,d);let n=Dn(t,e);return 1===r.rank&&(n=Ts(n,u)),In(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=Ts(e,u)),In(e,r.shape)}}}},Tr={kernelName:"GatherV2",inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=Yt(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Er(0,l),p=Er(l+1,l+1+h),d=Fr([o,[n],u]),f=In(t,d),g=In(i,[n]),m=Fr([[l],c,p]),y=Ri(f,m);let b=_i(y,g,s.shape[a]);const w=_s(m);return b=Ri(b,w),b},indices:()=>i}}};function Er(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Fr(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const _r={kernelName:"GreaterEqual",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>ns(n),b:()=>ns(s)}}},Lr={kernelName:"Identity",gradFunc:t=>({x:()=>yn(t,"float32")})},Rr={kernelName:"IsFinite",gradFunc:t=>({x:()=>ns(t)})},Mr={kernelName:"IsInf",gradFunc:t=>({x:()=>ns(t)})},Or={kernelName:"IsNan",gradFunc:t=>({x:()=>ns(t)})},Br={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=ds(s,0);return{x:()=>es(r,t,Dn(t,i))}}},Pr={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,on(n,1))}}},Wr={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,yn(n,"float32"))}}},Ur={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=os(s);return $s(t,Dn(Ts(t,i,!0),e))}}}};const Kr=nn({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Je.runKernel("LRNGrad",o,l)}}),jr={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>Kr(s,i,t,r,a,o,l)}}};function Vr(t,e,n,s){return e.rank<n.rank&&(e=In(e,Fs(e.shape,s))),t.rank<n.rank&&(t=In(t,Fs(t.shape,s))),{x:()=>Dn(t,yn(ts(n,e),t.dtype))}}const qr={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=Vr(t,e[1],r,Yt(i,r.shape));return{x:()=>a.x()}}},Gr={kernelName:"Maximum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Dn(t,yn(fs(n,s),"float32")),b:()=>Dn(t,yn(ks(n,s),"float32"))}}};const Hr=nn({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=tn(t,"dy","maxPool3dGrad"),l=tn(e,"input","maxPool3dGrad"),u=tn(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=In(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=In(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=In(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),Vt(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),Vt(5===c.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),Vt(5===p.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${p.rank}.`)),null!=a&&Vt(Jt(r),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=Je.runKernel("MaxPool3DGrad",f,g);return d?In(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Jr={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Hr(t,s,i,r,a,o,l)}}};const Zr=nn({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=tn(t,"dy","maxPoolGrad"),l=tn(e,"input","maxPoolGrad"),u=tn(n,"output","maxPoolGrad");Vt(l.rank===o.rank,(()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`)),Vt(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),Vt(4===l.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${l.rank}.`)),null!=a&&Vt(Jt(r),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return Je.runKernel("MaxPoolGrad",h,c)}}),Yr={kernelName:"PadV2",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>Tn(t,r,s.shape)}}};const Xr={kernelName:"SpaceToBatchND",gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>Fn(t,s,i)}}},Qr={kernelName:"SplitV",gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>Cn(t,s)}}};const ta=[ji,Vi,qi,Gi,Hi,Ji,Zi,Yi,Xi,Qi,tr,er,sr,rr,ar,or,lr,ur,hr,cr,pr,dr,gr,fr,yr,br,wr,kr,xr,vr,{kernelName:"RealDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{const e=Yn(t,yn(s,"float32")),r=Xn(n.shape,i);return r.length>0?In(Ts(e,r),n.shape):e},b:()=>{let e=Dn(t,yn(n,"float32"));const r=Xn(s.shape,i);r.length>0&&(e=In(Ts(e,r),s.shape));const a=Js(s);return As(Yn(e,yn(a,"float32")))}}}},Sr,Ir,Nr,Ar,Cr,Dr,zr,$r,Tr,_r,Lr,Rr,Mr,Or,Br,Pr,Wr,Ur,jr,qr,qr,Gr,Jr,{kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>Zr(t,s,i,r,a,o)}}},{kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=Yt(i,s.shape),a=Gt(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach((t=>{e[t]=1}));const n=In(t,e);return Yn(Dn(n,js(s.shape,"float32")),a)}}}},{kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=Vr(t,a,r,Yt(i,r.shape));return{x:()=>o.x()}}},{kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Dn(t,yn(xs(n,s),"float32")),b:()=>Dn(t,yn(ds(n,s),"float32"))}}},{kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>Tn(t,r,s.shape)}}},{kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{const e=Xn(n.shape,i);return e.length>0?In(Ts(t,e),n.shape):t},b:()=>{const e=Dn(t,As(cs(Yn(n,s)))),r=Xn(s.shape,i);return r.length>0?In(Ts(e,r),s.shape):e}}}},{kernelName:"Multiply",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{const e=Dn(t,yn(s,"float32")),r=Xn(n.shape,i);return r.length>0?In(Ts(e,r),n.shape):e},b:()=>{const e=Dn(t,yn(n,"float32")),r=Xn(s.shape,i);return r.length>0?In(Ts(e,r),s.shape):e}}}},{kernelName:"Neg",gradFunc:t=>({x:()=>As(t)})},{kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>Ks(n.shape,"float32")}}},{kernelName:"OnesLike",gradFunc:t=>({x:()=>ns(t)})},{kernelName:"Pack",saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return Li(t,s).map((t=>()=>t))}},Yr,Yr,{kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=Qn(r.shape,a.shape);return{a:()=>{const e=yn(a,"float32");let n=Dn(t,Dn(e,ni(r,$s(e,di(1)))));const s=Xn(r.shape,o);return s.length>0&&(n=Ts(n,s)),In(n,r.shape)},b:()=>{const e=ds(r,0),n=es(e,Ss(r),ns(r));let s=Dn(t,Dn(i,n));const l=Xn(a.shape,o);return l.length>0&&(s=Ts(s,l)),In(s,a.shape)}}}},{kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=ds(n,0);return{x:()=>es(i,t,Dn(t,s)),alpha:()=>{let e=es(i,ns(t),Dn(t,n));const r=Xn(s.shape,t.shape);return r.length>0&&(e=Ts(e,r)),In(e,s.shape)}}}},{kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,As(Js(n)))}}},{kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=Dn(xs(n,6),Di(n));return{x:()=>Dn(t,yn(s,"float32"))}}},{kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,yn(Di(n),"float32"))}}},{kernelName:"Reshape",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,n.shape)}}},{kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Je.runKernel("ResizeBilinearGrad",i,n)}}},{kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Je.runKernel("ResizeNearestNeighborGrad",i,n)}}},{kernelName:"Reverse",gradFunc:(t,e,n)=>{const{dims:s}=n,i=Yt(s,t.shape);return{x:()=>ui(t,i)}}},{kernelName:"Round",gradFunc:t=>({x:()=>ns(t)})},{kernelName:"Rsqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>As(Yn(t,Dn(ni(n,1.5),2)))}}},{kernelName:"Select",inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>yn(ns(n),"float32"),t:()=>Dn(t,yn(n,t.dtype)),e:()=>Dn(t,yn(Ms(n),t.dtype))}}},{kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=ds(n,di(0)),s=di(1.7580993408473768),i=di(1.0507009873554805),r=Dn(t,i),a=Dn(Dn(t,s),os(yn(n,"float32")));return es(e,r,a)}}}},{kernelName:"Sigmoid",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,Dn(n,$s(di(1),n)))}}},{kernelName:"Sign",gradFunc:t=>({x:()=>ns(t)})},{kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(jn(yn(n,"float32")),t)}}},{kernelName:"Sinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(Vn(yn(n,"float32")),t)}}},{kernelName:"Slice",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach((t=>{Vt(-1!==t,(()=>"slice() does not support negative begin indexing."))})),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map(((e,n)=>e>=0?e:(Vt(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-s[n]))),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>Qs(t,u)}}},{kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=Dn(t,s);return{logits:()=>$s(r,Dn(Ts(r,[i],true),s))}}},{kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,$n(n))}}},Xr,Xr,Qr,Qr,{kernelName:"Sqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Dn(Ni(yn(n,"float32")),2))}}},{kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=di(2);return{a:()=>Dn(t,Dn(i,$s(n,s))),b:()=>Dn(t,Dn(i,$s(s,n)))}}},{kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn(t,Dn(yn(n,"float32"),2))}}},{kernelName:"Step",gradFunc:t=>({x:()=>ns(t)})},{kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Qn(n.shape,s.shape);return{a:()=>{let e=t;const s=Xn(n.shape,i);return s.length>0&&(e=Ts(e,s)),In(e,n.shape)},b:()=>{let e=t;const n=Xn(s.shape,i);return n.length>0&&(e=Ts(e,n)),In(As(e),s.shape)}}}},{kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;Yt(r,s.shape).forEach((t=>{i[t]=1}));const a=In(t,i),o=Dn(a,js(s.shape,"float32"));return{x:()=>o}}},{kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Js(jn(n)))}}},{kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Dn($s(di(1),Js(n)),t)}}},{kernelName:"Tile",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=ns(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=on(e,Tn(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=on(e,Tn(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=on(e,Tn(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=on(e,Tn(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},{kernelName:"Transpose",gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=_s(i);return{x:()=>Ri(t,r)}}},{kernelName:"Unpack",gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>zi(t,i)}}},{kernelName:"UnsortedSegmentSum",inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=Ws(e,ns(e)),s=ps(t,n);let i=fs(e,di(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=ls(i,t+1);i=Rs(i,js(s.shape,"bool"));const a=ns(s);return es(i,s,a)}(t,n)}}},{kernelName:"ZerosLike",gradFunc:t=>({x:()=>ns(t)})}];for(const t of ta)xe(t);let ea;function na(){return null==ea&&(ea=t().epsilon()),ea}class sa extends Error{constructor(t){super(t),Object.setPrototypeOf(this,sa.prototype)}}class ia extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ia.prototype)}}class ra extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ra.prototype)}}class aa extends Error{constructor(t){super(t),Object.setPrototypeOf(this,aa.prototype)}}class oa extends Error{constructor(t){super(t),Object.setPrototypeOf(this,oa.prototype)}}function la(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function ua(t,e){if(!t)throw new oa(e)}function ha(t,e){let n=0;for(const s of t)s===e&&n++;return n}function ca(t){return 1===t.length?t[0]:t}function pa(t){return Array.isArray(t)?t:[t]}function da(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function fa(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let ga={};function ma(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function ya(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>ya(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?ya(e):t[n]=e.value)}}}function ba(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in ga)r=ga[i];else if(r=e[i],null==r)throw new ra(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new ra(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in ga?[o,l]=ga.className:a in e&&([o,l]=e[a]),null==o)throw new ra(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(ga))t[e]=ga[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},ga);for(const t of Object.keys(n))ga[t]=n[t];ya(r.config);const s=l(o,r.config,n,i);return ga=Object.assign({},e),s}{const t=Object.assign({},ga);for(const t of Object.keys(n))ga[t]=n[t];const e=new o(r.config);return ga=Object.assign({},t),e}}}function wa(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function ka(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function xa(t){if(null==t)throw new ra(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function va(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new ra(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function Sa(t,e,n=0,s=1/0){return ua(n>=0),ua(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function Ia(t,n){Array.isArray(t)?(e.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>Ia(t,`element ${e+1} of ${n}`)))):e.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${Na(t)}.`))}function Na(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>Na(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function Aa(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function Ca(t,e){return s((()=>i(r(a(t,t),e,!0))))}class za extends n.Serializable{getConfig(){return{}}}class Da extends za{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=Ca(t,this.axis),n=o(e,0,this.maxValue);return a(t,l(n,u(na(),e)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Da.className="MaxNorm",n.registerClass(Da);class $a extends za{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>l(t,u(na(),Ca(t,this.axis)))))}getConfig(){return{axis:this.axis}}}$a.className="UnitNorm",n.registerClass($a);class Ta extends za{apply(t){return h(t)}}Ta.className="NonNeg",n.registerClass(Ta);class Ea extends za{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=Ca(t,this.axis),n=u(a(this.rate,o(e,this.minValue,this.maxValue)),a(1-this.rate,e));return a(t,l(n,u(na(),e)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Ea.className="MinMaxNorm",n.registerClass(Ea);const Fa={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function _a(t){return ma(t)}function La(t,e={}){return ba(t,n.SerializationMap.getMap().classNameMap,e,"constraint")}function Ra(t){if(null==t)return null;if("string"==typeof t){return La({className:t in Fa?Fa[t]:t,config:{}})}return t instanceof za?t:La(t)}var Ma=Object.freeze({__proto__:null,maxNorm:function(t){return new Da(t)},unitNorm:function(t){return new $a(t)},nonNeg:function(){return new Ta},minMaxNorm:function(t){return new Ea(t)}});const Oa=["channelsFirst","channelsLast"],Ba=["nearest","bilinear"],Pa=["valid","same","causal"],Wa=["max","avg"],Ua=["sum","mul","concat","ave"],Ka=new Map;function ja(t){va(Oa,"DataFormat",t)}function Va(t){va(Pa,"PaddingMode",t)}function qa(t){va(Wa,"PoolMode",t)}const Ga=[];function Ha(t,e){Ga.push(t);try{const t=e();return Ga.pop(),t}catch(t){throw Ga.pop(),t}}function Ja(t){if(!Xa(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===Ga.length?"":Ga.join("/")+"/")+t}function Za(t){if(!Xa(t))throw new Error("Not a valid tensor name: '"+t+"'");Ka.has(t)||Ka.set(t,0);const e=Ka.get(t);if(Ka.set(t,Ka.get(t)+1),e>0){const n=`${t}_${e}`;return Ka.set(n,1),n}return t}const Ya=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function Xa(t){return!!t.match(Ya)}function Qa(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function to(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s<e&&(e=s)}return e}function eo(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s>e&&(e=s)}return e}function no(t,e){if(e<t)throw new ra(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function so(t,e){return t.asType(e)}function io(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),t.reshape(n)}function ro(t,e,n){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:return f(t,[e,0],[n,t.shape[1]]);case 3:return d(t,[e,0,0],[n,t.shape[1],t.shape[2]]);case 4:return p(t,[e,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3]]);case 5:return c(t,[e,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return c(t,[e,0,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new ra(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function ao(t,e,n){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:return f(t,[0,e],[t.shape[0],n]);case 3:return d(t,[0,0,e],[t.shape[0],t.shape[1],n]);case 4:return p(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],n]);default:throw new ra(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function oo(t,e,n,i){return s((()=>{switch(t.rank){case 1:return g(t,e,n);case 2:switch(i){case 1:return ro(t,e,n);case 2:return ao(t,e,n);default:throw new ra(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return ro(t,e,n);case 2:return d(t,[0,e,0],[t.shape[0],n,t.shape[2]]);case 3:return ao(t,e,n);default:throw new ra(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return ro(t,e,n);case 2:return p(t,[0,e,0,0],[t.shape[0],n,t.shape[2],t.shape[3]]);case 3:return p(t,[0,0,e,0],[t.shape[0],t.shape[1],n,t.shape[3]]);case 4:return ao(t,e,n);default:throw new ra(`The axis is not within the rank of the tensor ${i}`)}default:throw new ra(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function lo(t,e=-1){let n;return e<0&&(n=t[0].rank,e=0!==n?n:0),e===t[0].rank&&(e=-1),S(t,e)}function uo(t,e){switch(t.rank){case 1:return z([t,e]);case 2:return C([t,e],0);case 3:return A([t,e],0);case 4:return N([t,e],0);default:throw new ra(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function ho(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new ra(`The length of input n (${e.length}) does not match the number of dimensions in input x (${t.rank})`);return b(t,e)}function co(t,e=0,n=1,s,i){return w(t,e,n,s,i)}function po(t,e,n,s){if(t.rank<2||e.rank<2)throw new aa(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${e.shape}`);if(e.rank>=3){if(t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new aa(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${e.shape}`)}if(2===t.rank&&2===e.rank){const i=!1,r=!1;return I.matMul({a:t,b:e,transposeA:i,transposeB:r,bias:s?mo(t.rank,s,"channelsLast"):null,activation:n})}{const i=t.shape.slice(),r=i.pop();t=t.reshape([-1,r]);const a=e.shape.slice(),o=a.pop(),l=a.pop(),u=[...a,o],h=Array.from({length:e.rank},((t,n)=>0===n?e.rank-2:n<=e.rank-2?n-1:n));e=e.transpose(h).reshape([l,-1]);const c=[...i,...u],p=!1,d=!1;return I.matMul({a:t,b:e,transposeA:p,transposeB:d,bias:s?mo(t.rank,s,"channelsLast"):null,activation:n}).reshape(c)}}function fo(t,e,n){return s((()=>(e=Array.isArray(e)?m(e,"int32"):e.toInt(),y(t,e,n))))}function go(t){return a(t,t)}function mo(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new ra(`Unexpected bias dimensions: ${e.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1,1]):e.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,1,s[0]]):e.reshape([1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1]):e.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,s[0]]):e.reshape([1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1]):e.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,s[0]]):e.reshape([1].concat(s))}else if(t<3)return e;throw new ra(`Unsupported input rank by biasAdd: ${e.rank}`)}function yo(t,e,n){return s((()=>(null==n&&(n="channelsLast"),ja(n),t.add(mo(t.rank,e,n)))))}function bo(t,e,n,i){return s((()=>v(t,e,n,i)))}function wo(t,e,n=!1){return n?t():e()}const ko=["fanIn","fanOut","fanAvg"],xo=["normal","uniform","truncatedNormal"];class vo extends n.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class So extends vo{apply(t,e){return D(t,e)}}So.className="Zeros",n.registerClass(So);class Io extends vo{apply(t,e){return $(t,e)}}Io.className="Ones",n.registerClass(Io);class No extends vo{constructor(t){if(super(),"object"!=typeof t)throw new ra(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new ra(`config must have value set but got ${t}`);this.value=t.value}apply(t,e){return s((()=>a(T(this.value),$(t,e))))}getConfig(){return{value:this.value}}}No.className="Constant",n.registerClass(No);class Ao extends vo{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return E(t,this.minval,this.maxval,e)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}Ao.className="RandomUniform",n.registerClass(Ao);class Co extends vo{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new aa(`randomNormal does not support dType ${e}.`);return co(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Co.className="RandomNormal",n.registerClass(Co);class zo extends vo{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new aa(`truncatedNormal does not support dType ${e}.`);return F(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}zo.className="TruncatedNormal",n.registerClass(zo);class Do extends vo{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return s((()=>{if(2!==t.length||t[0]!==t[1])throw new ra("Identity matrix initializer can only be used for 2D square matrices.");return a(this.gain,_(t[0]))}))}getConfig(){return{gain:this.gain}}}Do.className="Identity",n.registerClass(Do);class $o extends vo{constructor(t){if(super(),t.scale<0)throw new ra(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,va(ko,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){va(xo,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const n=function(t,e="channelsLast"){let n,s;if(ja(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=Qa(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=Qa(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=Qa(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),s=n[0],i=n[1];let r=this.scale;if("fanIn"===this.mode?r/=Math.max(1,s):"fanOut"===this.mode?r/=Math.max(1,i):r/=Math.max(1,(s+i)/2),"normal"===this.distribution){const n=Math.sqrt(r);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new aa(`${this.getClassName()} does not support dType ${e}.`);return F(t,0,n,e,this.seed)}{const n=Math.sqrt(3*r);return E(t,-n,n,e)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}$o.className="VarianceScaling",n.registerClass($o);class To extends $o{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $o.className}}To.className="GlorotUniform",n.registerClass(To);class Eo extends $o{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $o.className}}Eo.className="GlorotNormal",n.registerClass(Eo);class Fo extends $o{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $o.className}}Fo.className="HeNormal",n.registerClass(Fo);class _o extends $o{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $o.className}}_o.className="HeUniform",n.registerClass(_o);class Lo extends $o{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $o.className}}Lo.className="LeCunNormal",n.registerClass(Lo);class Ro extends $o{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $o.className}}Ro.className="LeCunNormal",n.registerClass(Ro);class Mo extends vo{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new aa("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return s((()=>{if(t.length<2)throw new aa("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=co(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let n=L.gramSchmidt(e);return t[0]>t[1]&&(n=n.transpose()),a(this.gain,n)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}Mo.className="Orthogonal",n.registerClass(Mo);const Oo={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Bo(t,e={}){return ba(t,n.SerializationMap.getMap().classNameMap,e,"initializer")}function Po(t){return ma(t)}function Wo(t){if("string"==typeof t){const e=t in Oo?Oo[t]:t;if("GlorotNormal"===e)return new Eo;if("GlorotUniform"===e)return new To;if("HeNormal"===e)return new Fo;if("HeUniform"===e)return new _o;if("LeCunNormal"===e)return new Lo;if("LeCunUniform"===e)return new Ro;{const t={};return t.className=e,t.config={},Bo(t)}}return t instanceof vo?t:Bo(t)}var Uo=Object.freeze({__proto__:null,zeros:function(){return new So},ones:function(){return new Io},constant:function(t){return new No(t)},randomUniform:function(t){return new Ao(t)},randomNormal:function(t){return new Co(t)},truncatedNormal:function(t){return new zo(t)},identity:function(t){return new Do(t)},varianceScaling:function(t){return new $o(t)},glorotUniform:function(t){return new To(t)},glorotNormal:function(t){return new Eo(t)},heNormal:function(t){return new Fo(t)},heUniform:function(t){return new _o(t)},leCunNormal:function(t){return new Lo(t)},leCunUniform:function(t){return new Ro(t)},orthogonal:function(t){return new Mo(t)}});let Ko=0;function jo(){return Ko++}const Vo={};function qo(t=""){return t in Vo||(Vo[t]=0),Vo[t]+=1,t+Vo[t].toString()}function Go(t){return Array.isArray(t)&&Array.isArray(t[0])}function Ho(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function Jo(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new ra(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function Zo(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new ra(`Expected exactly 1 Shape; got ${t.length}`)}return t}function Yo(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}class Xo{constructor(t,e="float32",n="Variable",s=!0,i=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=jo(),n=null==n?"Variable":n,this.originalName=Ja(n),this.name=Za(this.originalName),this.trainable_=s,this.constraint=i,this.val=R(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function Qo(t){return t.map((t=>t.read()))}function tl(t){t.forEach((t=>{t[0].write(t[1])}))}class el{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class nl{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=jo(),null!=r&&(this.originalName=Ja(r),this.name=Za(this.originalName)),this.rank=e.length}}let sl=0;class il{constructor(t,e){this.callArgs=e,this.id=sl++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let rl=0;class al extends n.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=rl++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=da(t)+"_"+qo(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new ia(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new ra(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return ca(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return ca(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new sa(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new sa(`Layer ${this.name} is not connected, no input to return.`);return ca(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new sa(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new sa(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return ca(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=pa(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=pa(this.inputSpec);if(t.length!==e.length)throw new ra(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new ra(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new ra(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new ra(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new ra(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new ra(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new ra(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=pa(t);let s=!0;for(const t of n)if(!(t instanceof nl)){s=!1;break}let i=!0;for(const t of n)if(t instanceof nl){i=!1;break}if(s===i)throw new ra("Arguments to apply() must be all SymbolicTensors or all Tensors");return Ha(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of pa(t))e.push(n.shape);this.build(ca(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=pa(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=ca(r),null!=this.activityRegularizer)throw new aa("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=pa(t);const e=[];for(const n of t)e.push(n.shape);return ca(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new nl(r,n,this,pa(t),e,this.name,s))):new nl(r,s,this,pa(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new aa("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new sa(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new sa(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new ia(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return Yo(this.weights)}build(t){this.built=!0}getWeights(t=!1){return Qo(t?this.trainableWeights:this.weights)}setWeights(t){s((()=>{const n=this.weights;if(n.length!==t.length)throw new ra(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=Qo(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.arraysEqual(a.shape,l.shape))throw new ra(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}tl(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new ra(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=Wo("zeros"));const o=s.apply(e,n),l=new Xo(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=pa(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=pa(t);e=pa(e),n=pa(n),s=pa(s),i=Ho(i),r=Ho(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new il({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function ol(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=ol(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class ll extends al{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:qo("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new ra("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new ra("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new ra("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new nl(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new il({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new ra(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function ul(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new ra("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new ll({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function hl(t){if(null==t)return;const e=[],n=[],s=[];for(const i in t){const r=t[i];if("number"!=typeof r){const t=r;e.push(t.data()),n.push(i),s.push(t)}}if(e.length>0){const i=await Promise.all(e);for(let e=0;e<i.length;++e)t[n[e]]=i[e][0];M(s)}}function cl(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var pl;ll.className="InputLayer",n.registerClass(ll),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(pl||(pl={}));class dl{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class fl{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class gl extends dl{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const n=null==e.size?0:e.size;this.seen+=n;for(const t in e){const i=e[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*n;else{let e;t in this.totals?e=this.totals[t]:this.totals[t]=0;const r=s((()=>u(this.totals[t],a(i,n))));this.totals[t]=r,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?e[t]=this.totals[t]/this.seen:s((()=>{const n=a(l(1,this.seen),this.totals[t]);e[t]=n,this.totals[t].dispose(),B(e[t])})))}}class ml extends dl{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class yl extends dl{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.now();return(...r)=>{const a=e.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,n){const s=[];null!=this.yield&&(await hl(n),s.push(this.yield(t,e,n))),s.push(O()),await Promise.all(s)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await hl(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const n=[];null!=this.epochEnd&&(await hl(e),n.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&n.push(O()),await Promise.all(n)}async onBatchBegin(t,e){null!=this.batchBegin&&(await hl(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await hl(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(O()):e.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await hl(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await hl(t),await this.trainEnd(t))}}function bl(t,e){if(null==t&&(t={}),t instanceof dl)return[t];if(Array.isArray(t)&&t[0]instanceof dl)return t;return pa(t).map((t=>new yl(t,e)))}class wl{constructor(){}static registerCallbackConstructor(t,n){e.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),wl.checkForDuplicate(n),null==wl.constructors[t]&&(wl.constructors[t]=[]),wl.constructors[t].push(n)}static checkForDuplicate(t){for(const e in wl.constructors){wl.constructors[+e].forEach((e=>{if(e===t)throw new ra("Duplicate callback constructor.")}))}}static clear(){wl.constructors={}}static createCallbacks(t){const e=[];for(const n in wl.constructors){const s=+n;t>=s&&e.push(...wl.constructors[s])}return e.map((t=>new t))}}function kl(t,e,n,s,i,r,a,o,l){const u=new ml,h=[new gl,...wl.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new fl(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function xl(t,e={},s=!1){return ba(t,n.SerializationMap.getMap().classNameMap,e,"layer",s)}function vl(t,e){return s((()=>{"float32"!==t.dtype&&(t=t.asType("float32"));const n=r(go(t),e,!0),s=V(n.shape,na()),a=i(q(n,s));return l(t,a)}))}function Sl(t,e){return s((()=>U(go(W(e,t)),-1)))}function Il(t,e){return s((()=>U(x(W(e,t)),-1)))}function Nl(t,e){return s((()=>{const n=W(t,e),s=o(x(t),na(),Number.MAX_VALUE),i=x(l(n,s));return a(100,U(i,-1))}))}function Al(t,e,n=!1){return s((()=>{if(n)e=K(e);else{const t=r(e,e.shape.length-1,!0);e=l(e,t)}return e=o(e,na(),1-na()),j(r(a(t.toFloat(),P(e)),e.shape.length-1))}))}function Cl(t,e,n=!1){return s((()=>{const s=G(function(t){const e=[Qa(t.shape)];return t.reshape(e)}(t)).toInt(),i=(e=o(e,na(),1-na())).shape;return Al(H(s,i[i.length-1]).reshape(i),e,n)}))}function zl(t,n){return s((()=>{let i;return i=o(n,na(),1-na()),i=P(l(i,W(1,i))),U(function(t,n){if(!e.arraysEqual(t.shape,n.shape))throw new ra(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return s((()=>{const e=n.relu(),s=n.abs().neg();return e.sub(n.mul(t)).add(s.exp().log1p())}))}(t,i),-1)}))}function Dl(t,e){return s((()=>{const n=vl(t,-1),s=vl(e,-1),i=a(n,s);return j(r(i,-1))}))}wl.constructors={};const $l={meanSquaredError:Sl,meanAbsoluteError:Il,meanAbsolutePercentageError:Nl,meanSquaredLogarithmicError:function(t,e){return s((()=>{const n=o(e,na(),Number.MAX_VALUE),s=P(u(1,n)),i=o(t,na(),Number.MAX_VALUE),r=P(u(1,i));return U(go(W(s,r)),-1)}))},squaredHinge:function(t,e){return s((()=>{const n=q(0,W(1,a(t,e)));return U(go(n),-1)}))},hinge:function(t,e){return s((()=>{const n=q(0,W(1,a(t,e)));return U(n,-1)}))},categoricalHinge:function(t,e){return s((()=>{const n=r(a(t,e),-1),s=J(a(W(1,t),e),-1);return q(0,u(1,W(s,n)))}))},logcosh:function(t,e){return s((()=>{const n=Math.log(2),s=W(e,t),i=W(u(s,Z(a(-2,s))),n);return U(i,-1)}))},categoricalCrossentropy:Al,sparseCategoricalCrossentropy:Cl,binaryCrossentropy:zl,kullbackLeiblerDivergence:function(t,e){return s((()=>{const n=o(t,na(),1),s=o(e,na(),1);return r(a(t,P(l(n,s))),-1)}))},poisson:function(t,e){return s((()=>{const n=P(u(na(),e));return U(W(e,a(t,n)),-1)}))},cosineProximity:Dl};function Tl(t){if("string"==typeof t){if(t in $l)return $l[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new ra(e)}return t}function El(t,e){return s((()=>{const n=a(.5,Y(e)),s=so(X(e,n),t.dtype);return U(Q(t,s),-1)}))}function Fl(t,e){return s((()=>so(Q(tt(t,-1),tt(e,-1)),"float32")))}function _l(t,e){return s((()=>et(t.equal(1),e.equal(1)).sum().cast("float32")))}function Ll(t,e){return s((()=>{const n=_l(t,e),i=function(t,e){return s((()=>et(t.equal(0),e.equal(1)).sum().cast("float32")))}(t,e),r=n.add(i);return nt(X(r,0),n.div(r),0).cast("float32")}))}function Rl(t,e){return s((()=>{const n=_l(t,e),i=function(t,e){return s((()=>et(t.equal(1),e.equal(0)).sum().cast("float32")))}(t,e),r=n.add(i);return nt(X(r,0),n.div(r),0).cast("float32")}))}function Ml(t,e){return zl(t,e)}function Ol(t,e){return t.rank===e.rank&&(t=t.squeeze([t.rank-1])),(e=e.argMax(-1)).dtype!==t.dtype&&(e=e.asType(t.dtype)),Q(t,e).asType("float32")}const Bl=Al,Pl=Cl,Wl={binaryAccuracy:El,categoricalAccuracy:Fl,precision:Ll,categoricalCrossentropy:Bl,sparseCategoricalCrossentropy:Pl,mse:Sl,MSE:Sl,mae:Il,MAE:Il,mape:Nl,MAPE:Nl,cosine:Dl};function Ul(t){if("string"==typeof t&&t in Wl)return Wl[t];if("string"!=typeof t&&null!=t)return t;throw new ra(`Unknown metric ${t}`)}function Kl(t){if(ua(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys($l))if($l[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(Wl))if(Wl[n]===t){e=n;break}return void 0!==e?e:t.name}}function jl(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!Vl(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function Vl(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!Vl(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!Vl(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function ql(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),Gl(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?Hl(o[t],n,s):Jl(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?Yo(t.collectedTrainableWeights):Yo(t.trainableWeights);return e}(t),u=Yo(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function Gl(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function Hl(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}Gl([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function Jl(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];Gl([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)Gl(["","","",r[t]],e,s)}function Zl(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function Yl(t,e){if(null===t)return null;if("string"==typeof t)return fa(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Zl(e,i,s)?n.push(s):n.push(Yl(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=fa(n);e[t]=Yl(s,t)}}return e}}function Xl(t,e){if(null==t)return null;if("string"==typeof t)return da(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Zl(e,i,s)?n.push(s):n.push(Xl(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=da(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?Xl(s,n):s}return e}}const Ql="3.7.0";class tu{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof tu)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new ra(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return rt(e,t.dtype)}catch(n){throw new ra(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof nl){if(null==this.id2Value[t.id])throw new ra(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new ra(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof nl){if(null==this.id2Value[t.id])throw new ra(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new ra(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&M(this.id2Mask)}}const eu={},nu={};function su(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==eu[c]){const t=function(t,n){e.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=ru(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=ru(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:iu(i)}}(o,n);p=t.sorted,d=t.recipientCounts,eu[c]=p,nu[c]=d}p=eu[c],d={},r||Object.assign(d,nu[c]);const f=new tu(n);for(let t=0;t<p.length;++t){if(null!=i){const t=it().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const e=p[t],a=e.sourceLayer;if(a instanceof ll)continue;const o=[],h=[],c=[];let g=!1;for(const t of e.inputs){const e=f.getValue(t),s=f.getMask(t);o.push(e),h.push(s),null!=s&&(g=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||c.push(e))}g&&((s=s||{}).mask=h[0]);const m=pa(a.apply(o,s));let y=null;a.supportsMasking&&(y=a.computeMask(o,h));const b=au(e),w=Array.isArray(b)?b:[b];for(let t=0;t<w.length;++t){f.hasKey(w[t])||f.add(w[t],m[t],Array.isArray(y)?y[0]:y);const e=l.indexOf(w[t].name);-1!==e&&(u[e]=m[t])}r||M(c)}return f.disposeMasks(),a?u:u[0]}function iu(t){const e={};for(const n in t)e[n]=t[n].size;return e}function ru(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function au(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class ou extends al{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=qo(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],ka(this.inputs).length!==this.inputs.length)throw new ra(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);ka(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;ua(0===n,"input layer has >1 nodes"),ua(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof ll))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new ia(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(ou.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(wa);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof ou&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort(wa);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new ia(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new ia(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new il({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new ra("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new ra(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new ra(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new ra(`${t.length} of ${s} weights are not set: ${t}`)}tl(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.7.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=Xl(this.updatedConfig());return e?JSON.stringify(n):n}call(t,e){return s((()=>{t=pa(t);const n=new tu;for(let e=0;e<this.inputs.length;++e)n.add(this.inputs[e],t[e]);return su(this.outputs,n,e)}))}computeMask(t,e){return s((()=>{let n;return t=pa(t),n=null==e?la(null,t.length):pa(e),this.runInternalGraph(t,n)[1]}))}computeOutputShape(t){const e=Ho(t);if(e.length!==this.inputLayers.length)throw new ra(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(wa);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=Ho(e.computeOutputShape(ca(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];ua(e in n),i.push(n[e])}return ca(i)}runInternalGraph(t,e){null==e&&(e=la(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(wa);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=pa(e.call(t,u)),l=pa(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=pa(e.call(s,u)),l=pa(e.computeMask(s,a));if(e.activityRegularizer)throw new aa("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){ua(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof ou?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=ou.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new ra(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new ra("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new ra(`No such layer: ${t}`)}calculateLosses(){return s((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=ou.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=ou.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[ou.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=ou.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=ou.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(ca(n),s)}function l(t){const n=t.name,r=xl(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new ra(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!xa(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];ua(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];ua(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new ra("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){s((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function lu(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function uu(t,e,n,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const e=s((()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await e.data());M(e);const r=[];return i.forEach((t=>{if(null==n[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(n[t])})),m(r,"float32")}return null}function hu(t,e){return a(t,e)}function cu(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=pu("input",t.inputNames,s),o=pu("output",t.outputNames,i),l=a[0].shape[0];e.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function pu(t,n,s){if(s instanceof at)return[s];if(Array.isArray(s))return e.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new ra(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function du(t,n,s){const i=null!=s.batchesPerEpoch;if(e.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(fu(s.validationData))e.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new aa("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=bl(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=kl(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const e={};await d.onEpochBegin(g);let h=0,c=0;for(i||(m=await n.iterator());!i||h<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${h} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:e,ys:i}=cu(t,n.value),r={};r.batch=c,r.size=e[0].shape[0],await d.onBatchBegin(c,r);const a=[];if(null!=s.classWeight){const e=lu(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)a.push(await uu(i[t],null,e[t]))}const o=e.concat(i).concat(a),p=l(o);M(o);for(let t=0;t<u.length;++t){const e=u[t],n=p[t];r[e]=n,B(n)}await d.onBatchEnd(c,r),cl(r),c++,h++}if(i?h>=s.batchesPerEpoch:n.done){if(r){let n;n=fu(s.validationData)?pa(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):pa(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)e[`val_${t.metricsNames[s]}`]=n[s]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,e),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function fu(t){return"function"==typeof t.iterator}function gu(t){e.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function mu(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>ro(t,e,n-e))):ro(t,e,n-e)}function yu(t,e){return s((()=>null==t?null:Array.isArray(t)?t.map((t=>yu(t,e))):fo(t,"int32"===e.dtype?e:e.toInt())))}function bu(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function wu(t,n,i,r={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let a,o,l,u,h,c,p;t.isTraining=!0;try{const d=null==r.batchSize?32:r.batchSize;gu(d);const f=!1,g=await t.standardizeUserData(n,i,r.sampleWeight,r.classWeight,f,d);a=g[0],o=g[1],p=g[2];let y,b=!1;if(null!=r.validationData&&r.validationData.length>0){if(b=!0,2!==r.validationData.length)throw 3===r.validationData.length?new aa("validationData including sample weights is not supported yet."):new ra(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${r.validationData} is invalid.`);l=r.validationData[0],u=r.validationData[1];const e=!0,n=await t.standardizeUserData(l,u,null,null,e,d);h=n[0],c=n[1],y=h.concat(c)}else if(null!=r.validationSplit&&r.validationSplit>0&&r.validationSplit<1){b=!0;const t=Math.floor(a[0].shape[0]*(1-r.validationSplit)),e=a[0].shape[0];h=mu(a,t,e),a=mu(a,0,t),c=mu(o,t,e),o=mu(o,0,t),y=h.concat(c)}else null!=r.validationSteps&&(b=!0);const w=a.concat(o).concat(p);t.checkTrainableWeightsConsistency();const k=t.makeTrainFunction(),x=t.getDedupedMetricsNames();let v,S;b?(t.makeTestFunction(),v=t.testFunction,S=x.slice().concat(x.map((t=>"val_"+t)))):(v=null,y=[],S=x.slice());const I=bl(r.callbacks,r.yieldEvery);return await async function(t,n,i,r,a,o,l,u,h,c,p,d,f,g,y){null==a&&(a=32),null==o&&(o=1),null==p&&(p=!0),null==f&&(f=0);let b=!1;if(null!=h&&null!=c&&(b=!0),null!=y&&(b=!0,null==g))throw new ra("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const w=t.checkNumSamples(i,a,g,"steps_per_epoch");let k;null!=w&&(k=no(0,w)),null==l&&(l=1);const{callbackList:x,history:v}=kl(u,l,o,f,w,g,a,b,d);x.setModel(t),t.history=v,await x.onTrainBegin(),t.stopTraining_=!1;for(let l=f;l<o;++l){await x.onEpochBegin(l);const o={};if(null!=g)throw new aa("stepsPerEpoch mode is not implemented yet.");{if("batch"===p)throw new aa("batch shuffling is not implemneted yet");p&&e.shuffle(k);const l=m(k),u=bu(w,a);for(let e=0;e<u.length;++e){const p={};if(await x.onBatchBegin(e,p),s((()=>{const s=u[e][0],d=u[e][1],f=ro(l,s,d-s);p.batch=e,p.size=d-s;const g=yu(i,f),m=n(g);for(let t=0;t<r.length;++t){const e=r[t],n=m[t];p[e]=n,B(n)}if(e===u.length-1&&b){const e=t.testLoop(h,c,a);for(let t=0;t<r.length;++t){const n=r[t],s=e[t];B(s),o["val_"+n]=s}}})),await x.onBatchEnd(e,p),cl(p),t.stopTraining_)break}l.dispose()}if(await x.onEpochEnd(l,o),t.stopTraining_)break}return await x.onTrainEnd(),await t.history.syncData(),t.history}(t,k,w,x,d,r.epochs,r.verbose,I,v,y,r.shuffle,S,r.initialEpoch,null,null)}finally{t.isTraining=!1,xu(a,n),xu(o,i),xu(h,l),xu(c,u),null!=p&&M(p)}}function ku(t){const e=[];t instanceof at&&(t=[t]);for(let n=0;n<t.length;++n){const s=t[n];if(1===s.rank)e.push(io(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(s)}}return e}function xu(t,e){if(null==t)return;const n=[];if(e instanceof at)n.push(e.id);else if(Array.isArray(e))e.forEach((t=>n.push(t.id)));else if(null!=e)for(const t in e){const s=e[t];n.push(s.id)}const s=[];if(t instanceof at)-1===n.indexOf(t.id)&&s.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===n.indexOf(t.id)&&s.push(t)}));else if(null!=t)for(const e in t){const i=t[e];-1===n.indexOf(i.id)&&s.push(i)}s.forEach((t=>{t.isDisposed||t.dispose()}))}function vu(t){return Array.isArray(t)}function Su(t){return!function(t){return t instanceof at}(t)&&!vu(t)}function Iu(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(vu(t)&&t.length>0)e=!0;else if(Su(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new ra(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(Su(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new ra(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(vu(t)){if((t=t).length!==e.length)throw new ra(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new ra(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=ku(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new ra(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new ra(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function Nu(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new ra(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new ra(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new ra(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new ra(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class Au extends ou{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new ra("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");ql(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const e={Adagrad:()=>st.adagrad(.01),Adadelta:()=>st.adadelta(1,.95,na()),Adam:()=>st.adam(.001,.9,.999,na()),Adamax:()=>st.adamax(.002,.9,.999,na(),0),RMSProp:()=>st.rmsprop(.001,.9,0,na()),SGD:()=>st.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new ra(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof ot))throw new ra("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new ra(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const n=t.loss;e=n.map((t=>Tl(t)))}else{const n=Tl(t.loss);this.outputs.forEach((t=>{e.push(n)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new ra(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const n of this.outputNames)null==t.loss[n]&&console.warn(`Output "${n}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${n} during training`),e.push(Tl(t.loss[n]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const n=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Ha("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const s=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),i=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};Ha("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;(e=>{let n,s,r;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let i;1===e[e.length-1]||this.lossFunctions[t]===zl?-1!==["accuracy","acc"].indexOf(a)?s=El:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ml):this.lossFunctions[t]===Cl?-1!==["accuracy","acc"].indexOf(a)?s=Ol:-1!==["crossentropy","ce"].indexOf(a)&&(s=Pl):-1!==["accuracy","acc"].indexOf(a)?s=Fl:-1!==["crossentropy","ce"].indexOf(a)&&(s=Bl),-1!==["accuracy","acc"].indexOf(a)?i="acc":-1!==["crossentropy","ce"].indexOf(a)&&(i="ce"),r=s,n=""+i}else{const t=Ul(a);r=t,n=""+Kl(a)}let e;Ha(n,(()=>{e=r})),i(t,n,e)}})(s[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;gu(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return ca(this.testLoop(a,r,s,n.verbose,n.steps))}finally{xu(i[0],t),xu(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,i){const r=null!=(i=i||{}).batches,o=t.testFunction;let h=[];if(i.verbose>0)throw new aa("Verbose mode is not implemented yet.");e.assert(!r||i.batches>0&&Number.isInteger(i.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(i.batches)}`));const c="function"==typeof n.next?n:await n.iterator();let p=0,d=0;for(;!r||d<i.batches;){const e=await c.next();if(h=s((()=>{if(e.value){const{xs:n,ys:i}=cu(t,e.value),r=n.concat(i),l=s((()=>o(r)));if(M(r),0===d)for(let t=0;t<l.length;++t)h.push(T(0));const c=r[0].shape[0];for(let t=0;t<l.length;++t){const e=l[t],n=h[t];h[t]=s((()=>u(h[t],a(c,e)))),d>0&&M(n)}M(l),p+=c,++d}return h})),e.done){r&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${i.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<h.length;++t){const e=h[t];h[t]=l(h[t],p),M(e)}return ca(h)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new ra(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new ra(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new ra("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(e),s=n?e:[e],i=this.retrieveSymbolicTensors(s),r=new tu;if(t instanceof at&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new ra(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new ra(`No value is provided for the model's input ${e.name}`);r.add(e,n)}const a=su(i,r);return n?a:a[0]}retrieveSymbolicTensors(t){const e=la(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new ra(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,e=32,n=!1){return s((()=>{const i=this.checkNumSamples(t);if(n)throw new aa("Verbose predictLoop() is not implemented yet.");const r=bu(i,e),a=this.outputs.map((t=>[]));for(let e=0;e<r.length;++e){s((()=>{const n=r[e][0],s=r[e][1],i=mu(t,n,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new tu(a);return su(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return ca(a.map((t=>S(t,0))))}))}predict(t,e={}){const n=ku(t);Nu(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return gu(s),this.predictLoop(n,s)}finally{xu(n,t)}}predictOnBatch(t){Nu(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new ia("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Cl?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=ka(t.map((t=>t.shape[0])));i.sort();const r=ka(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new ra(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new ra(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.arraysEqual(i,r))throw new ra(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=Iu(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=Iu(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[Sl,zl,Al];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===Al&&1===r.shape[r.shape.length-1])throw new ra(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new ra(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new ra(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=lu(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await uu(o[e],null,t[e]))}return[a,o,l]}testLoop(t,e,n,i=0,r){return s((()=>{const s=this.checkNumSamples(e,n,r,"steps"),o=[];if(i>0)throw new aa("Verbose mode is not implemented yet.");if(null!=r)throw new aa("steps mode in testLoop() is not implemented yet");{const i=bu(s,n),r=m(no(0,s));for(let n=0;n<i.length;++n){const s=i[n][0],l=i[n][1],h=ro(r,s,l-s),c=yu(e,h),p=t(c);if(0===n)for(let t=0;t<p.length;++t)o.push(T(0));for(let t=0;t<p.length;++t){const e=p[t];o[t]=u(o[t],a(l-s,e))}}for(let t=0;t<o.length;++t)o[t]=l(o[t],s)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(ha(t,s)>1){i+=`_${ha(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],n=t.slice(0,this.inputs.length),s=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),i=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],a=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:n[e]});const a=new tu(t),o=su(this.outputs,a,{training:!0});let l;for(let t=0;t<this.lossFunctions.length;++t){let n=(0,this.lossFunctions[t])(s[t],o[t]);null!=i[t]&&(n=hu(n,i[t]));const r=U(n);e.push(r),l=0===t?n:u(l,n)}for(let t=0;t<this.metricsTensors.length;++t){let n;if(this.outputs.length>1&&t<this.outputs.length)n=e[t];else{const e=this.metricsTensors[t][0],i=this.metricsTensors[t][1];n=U(e(s[i],o[i]))}B(n),r.push(n)}return l=U(l),this.calculateLosses().forEach((t=>{l=u(l,t)})),l}),!0,a)].concat(r)}}makeTestFunction(){this.testFunction=t=>s((()=>{const e=[];let n;const s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:s[t]});const a=new tu(r),o=su(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const s=this.lossFunctions[t],r=U(s(i[t],o[t]));n=0===t?r:u(n,r),e.push(n)}for(let t=0;t<this.metricsTensors.length;++t){const n=this.metricsTensors[t][0],s=this.metricsTensors[t][1],r=U(n(i[s],o[s]));e.push(r)}return e}))}async fit(t,e,n={}){return wu(this,t,e,n)}async fitDataset(t,e){return du(this,t,e)}async trainOnBatch(t,e){const n=await this.standardizeUserData(t,e),s=n[0],i=n[1],r=this.makeTrainFunction()(s.concat(i)),a=[];for(const t of r){const e=await t.data();a.push(e[0])}return M(r),ca(a)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=it().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-it().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=da(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>da(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=da(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[da(Kl(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>da(Kl(t))));{const t={};for(const e in this.metrics)t[e]=da(Kl(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=xl(Yl(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=fa(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>fa(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=fa(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>fa(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=fa(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,e){if("string"==typeof t){const e=lt.getSaveHandlers(t);if(0===e.length)throw new ra(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new ra(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new ra("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await lt.encodeWeights(this.getNamedWeights(e)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.7.0",convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:i}=await lt.encodeWeights(await this.optimizer.getWeights(),t);n.specs.push(...i),n.data=lt.concatenateArrayBuffers([n.data,e])}if(null!=this.userDefinedMetadata){const t=!0;jl(this.userDefinedMetadata,this.name,t),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,t.save(s)}setUserDefinedMetadata(t){jl(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Au.className="Model",n.registerClass(Au);class Cu extends Au{}async function zu(t,e){if(null==e&&(e={}),"string"==typeof t){const n=lt.getLoadHandlers(t,e);if(0===n.length)n.push(lt.browserHTTPRequest(t,e));else if(n.length>1)throw new ra(`Found more than one (${n.length}) load handlers for URL '${t}'`);t=n[0]}return async function(t,e,n){null==n&&(n={});if(null==t.load)throw new ra("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await t.load();let i=s.modelTopology;null!=i.model_config&&(i=i.model_config);const r=null==n.strict||n.strict,a=null!=s.weightData&&null!=s.weightSpecs&&r,o=xl(Yl(i),e,a),l=s.trainingConfig;null!=l&&o.loadTrainingConfig(l);null!=s.userDefinedMetadata&&o.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new ra("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const n=lt.decodeWeights(t,e),s={},i=[];return e.forEach((t=>{"optimizer"===t.group?i.push({name:t.name,tensor:n[t.name]}):s[t.name]=n[t.name]})),{modelWeights:s,optimizerWeights:i}}(s.weightData,s.weightSpecs);o.loadWeights(t,r),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),M(t),M(e.map((t=>t.tensor)))}return o}(t,void 0,e)}Cu.className="Functional",n.registerClass(Cu);class Du extends Au{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:qo("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new ra(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof Du||t instanceof Au;let n;if(e){if(n=t,1!==n.outputs.length)throw new ra("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new ra("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new ra("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=ul({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new ra(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new ra("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=ol(this.outputs[0])}this.inboundNodes=[],new il({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:la(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(Zo(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Au({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new ia("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new ia("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new ia("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new ia("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new ra("Legacy serialization format not supported yet.");r=n}else e.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof Du))throw new aa(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=xl(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new ra("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new ra("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function $u(t){return new Au(t)}function Tu(t){return new Du(t)}function Eu(t,e){return null==e&&(e={}),zu(t,e)}function Fu(t){return ul(t)}function _u(t,e){wl.registerCallbackConstructor(t,e)}Du.className="Sequential",n.registerClass(Du);class Lu extends n.Serializable{getConfig(){return{}}}class Ru extends Lu{apply(t,e=1){return function(t,e=1){if(1!==e)throw new aa(`Support for alpha values other than 1 (${e}) is not implemented yet.`);return k(t)}(t,e)}}Ru.className="elu",n.registerClass(Ru);class Mu extends Lu{apply(t){return ut(t)}}Mu.className="selu",n.registerClass(Mu);class Ou extends Lu{apply(t){return h(t)}}Ou.className="relu",n.registerClass(Ou);class Bu extends Lu{apply(t){return s((()=>ht(6,h(t))))}}Bu.className="relu6",n.registerClass(Bu);class Pu extends Lu{apply(t){return t}}Pu.className="linear",n.registerClass(Pu);class Wu extends Lu{apply(t){return ct(t)}}Wu.className="sigmoid",n.registerClass(Wu);class Uu extends Lu{apply(t){return function(t){return s((()=>{const e=u(.5,a(.2,t));return o(e,0,1)}))}(t)}}Uu.className="hardSigmoid",n.registerClass(Uu);class Ku extends Lu{apply(t){return Z(t)}}Ku.className="softplus",n.registerClass(Ku);class ju extends Lu{apply(t){return function(t){return s((()=>l(t,x(t).add(1))))}(t)}}ju.className="softsign",n.registerClass(ju);class Vu extends Lu{apply(t){return pt(t)}}Vu.className="tanh",n.registerClass(Vu);class qu extends Lu{apply(t,e=-1){return K(t,e)}}qu.className="softmax",n.registerClass(qu);class Gu extends Lu{apply(t,e=-1){return dt(t,e)}}Gu.className="logSoftmax",n.registerClass(Gu);class Hu extends Lu{apply(t,e=1){return s((()=>ct(t.mul(e)).mul(t)))}}Hu.className="swish",n.registerClass(Hu);class Ju extends Lu{apply(t){return s((()=>a(t,pt(Z(t)))))}}function Zu(t){return t.getClassName()}function Yu(t,e={}){return ba(t,n.SerializationMap.getMap().classNameMap,e,"activation")}function Xu(t){if(null==t){const t={className:"linear",config:{}};return Yu(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},Yu(e)}return t instanceof Lu?t:Yu(t)}function Qu(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}Ju.className="mish",n.registerClass(Ju);class th extends n.Serializable{}class eh extends th{constructor(t){super(),Qu(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return s((()=>{let e=D([1]);return this.hasL1&&(e=u(e,r(a(this.l1,x(t))))),this.hasL2&&(e=u(e,r(a(this.l2,go(t))))),e.asScalar()}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}eh.className="L1L2",n.registerClass(eh);const nh={l1l2:"L1L2"};function sh(t){return ma(t)}function ih(t,e={}){return ba(t,n.SerializationMap.getMap().classNameMap,e,"regularizer")}function rh(t){if(null==t)return null;if("string"==typeof t){return ih({className:t in nh?nh[t]:t,config:{}})}return t instanceof th?t:ih(t)}class ah extends al{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=Jo(t);let n=h(t);return null!=this.maxValue&&(n=o(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}ah.className="ReLU",n.registerClass(ah);class oh extends al{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Jo(t);return ft(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}oh.className="LeakyReLU",n.registerClass(oh);class lh extends al{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=Wo(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=rh(t.alphaRegularizer),this.alphaConstraint=Ra(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new ra(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=Zo(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new el({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=Jo(t),gt(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Po(this.alphaInitializer),alphaRegularizer:sh(this.alphaRegularizer),alphaConstraint:_a(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}lh.className="PReLU",n.registerClass(lh);class uh extends al{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new aa(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Jo(t);return k(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}uh.className="ELU",n.registerClass(uh);class hh extends al{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=Jo(t);return n.mul(so(n.greater(this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}hh.className="ThresholdedReLU",n.registerClass(hh);class ch extends al{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new qu).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=Jo(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function ph(t,e,n){if("number"==typeof t)return la(t,e);if(t.length!==e)throw new ra(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new ra(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function dh(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function fh(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+eo([n-e,0]);else{if("same"!==s)throw new ra(`Unsupport padding mode: ${s}.`);t*=e}return t}function gh(t,e){return s((()=>(ja(e),"channelsFirst"===e?mt(t,[0,2,3,1]):t)))}function mh(t,e){return s((()=>(ja(e),"channelsFirst"===e?mt(t,[0,2,3,4,1]):t)))}function yh(t,e,n,i=[1,1],r="valid",a,o,l=null){return s((()=>{if(null==a&&(a="channelsLast"),ja(a),3!==t.rank&&4!==t.rank)throw new ra(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new ra(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let s=gh(t,a);if("causal"===r)throw new aa("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return s=I.conv2d({x:s,filter:e,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:n,activation:l}),"channelsFirst"===a&&(s=mt(s,[0,3,1,2])),s}))}ch.className="Softmax",n.registerClass(ch);class bh extends al{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",bh.verifyArgs(e),this.rank=t,Ia(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new aa(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=ph(e.kernelSize,t,"kernelSize"),this.strides=ph(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,Va(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,ja(this.dataFormat),this.activation=Xu(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=Wo(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Ra(e.biasConstraint),this.biasRegularizer=rh(e.biasRegularizer),this.activityRegularizer=rh(e.activityRegularizer),this.dilationRate=ph(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new ra(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new ra(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new ra(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(ua("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!Sa(t.kernelSize,"number",1,3))throw new ra(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:Zu(this.activation),useBias:this.useBias,biasInitializer:Po(this.biasInitializer),biasRegularizer:sh(this.biasRegularizer),activityRegularizer:sh(this.activityRegularizer),biasConstraint:_a(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class wh extends bh{constructor(t,e){super(t,e),this.kernel=null,wh.verifyArgs(e),this.filters=e.filters,Ia(this.filters,"filters"),this.kernelInitializer=Wo(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Ra(e.kernelConstraint),this.kernelRegularizer=rh(e.kernelRegularizer)}build(t){t=Zo(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new ra(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,e){return s((()=>{let e;t=Jo(t);const n=null==this.bias?null:this.bias.read(),i=Aa(this.activation.getClassName());if(null!=i&&2===this.rank)e=yh(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)e=function(t,e,n,i=1,r="valid",a,o=1){return s((()=>{if(null==a&&(a="channelsLast"),ja(a),3!==t.shape.length)throw new ra(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==e.shape.length)throw new ra(`The kernel for a conv1dWithBias operation should be 3, but is ${e.shape.length} instead`);if(null!=n&&1!==n.shape.length)throw new ra(`The bias for a conv1dWithBias operation should be 1, but is ${e.shape.length} instead`);if("channelsFirst"===a&&(t=mt(t,[0,2,1])),"causal"===r)throw new aa("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let s=wt(t,e,i,"same"===r?"same":"valid","NWC",o);return null!=n&&(s=yo(s,n)),s}))}(t,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=yh(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new aa("convolutions greater than 3D are not implemented yet.");e=function(t,e,n,i=[1,1,1],r="valid",a,o){return s((()=>{if(null==a&&(a="channelsLast"),ja(a),4!==t.rank&&5!==t.rank)throw new ra(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==e.rank&&5!==e.rank)throw new ra(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let s=mh(t,a);if("causal"===r)throw new aa("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return s=kt(s,e,i,"same"===r?"same":"valid","NDHWC",o),null!=n&&(s=yo(s,n)),"channelsFirst"===a&&(s=mt(s,[0,4,1,2,3])),s}))}(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=Zo(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=dh(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Po(this.kernelInitializer),kernelRegularizer:sh(this.kernelRegularizer),kernelConstraint:_a(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new ra(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class kh extends wh{constructor(t){super(2,t),kh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Sa(t.kernelSize,"number",1,2))throw new ra(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}kh.className="Conv2D",n.registerClass(kh);class xh extends wh{constructor(t){super(3,t),xh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new ra(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}xh.className="Conv3D",n.registerClass(xh);class vh extends kh{constructor(t){if(super(t),this.inputSpec=[new el({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new ra(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=Zo(t)).length)throw new ra("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new ra("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new el({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=Jo(t);if(4!==e.shape.length)throw new ra(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r;"channelsFirst"===this.dataFormat?(i=2,r=3):(i=1,r=2);const a=n[i],o=n[r],l=this.kernelSize[0],u=this.kernelSize[1],h=this.strides[0],c=this.strides[1],p=[s,fh(a,h,l,this.padding),fh(o,c,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=mt(e,[0,2,3,1]));let d=yt(e,this.kernel.read(),p,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(d=mt(d,[0,3,1,2])),null!=this.bias&&(d=yo(d,this.bias.read(),this.dataFormat)),null!=this.activation&&(d=this.activation.apply(d)),d}))}computeOutputShape(t){const e=(t=Zo(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=fh(e[s],o,r,this.padding),e[i]=fh(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}vh.className="Conv2DTranspose",n.registerClass(vh);class Sh extends xh{constructor(t){if(super(t),this.inputSpec=[new el({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new ra(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=Zo(t)).length)throw new ra("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new ra("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new el({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=Jo(t);if(5!==e.shape.length)throw new ra(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r,a;"channelsFirst"===this.dataFormat?(a=2,i=3,r=4):(a=1,i=2,r=3);const o=n[a],l=n[i],u=n[r],h=this.kernelSize[0],c=this.kernelSize[1],p=this.kernelSize[2],d=this.strides[0],f=this.strides[1],g=this.strides[2],m=[s,fh(o,d,h,this.padding),fh(l,f,c,this.padding),fh(u,g,p,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=mt(e,[0,2,3,4,1]));let y=bt(e,this.kernel.read(),m,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(y=mt(y,[0,4,1,2,3])),null!==this.bias&&(y=yo(y,this.bias.read(),this.dataFormat)),null!==this.activation&&(y=this.activation.apply(y)),y}))}computeOutputShape(t){const e=(t=Zo(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=fh(e[s],u,a,this.padding),e[i]=fh(e[i],h,o,this.padding),e[r]=fh(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Sh.className="Conv3DTranspose",n.registerClass(Sh);class Ih extends wh{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new ra("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new ra("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new ra(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=Wo(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=rh(e.depthwiseRegularizer),this.depthwiseConstraint=Ra(e.depthwiseConstraint),this.pointwiseInitializer=Wo(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=rh(e.pointwiseRegularizer),this.pointwiseConstraint=Ra(e.pointwiseConstraint)}build(t){if((t=Zo(t)).length<this.rank+2)throw new ra(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new ra(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new el({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e;if(t=Jo(t),1===this.rank)throw new aa("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=mt(t,[0,2,3,1])),e=xt(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=yo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=mt(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Po(this.depthwiseInitializer),t.pointwiseInitializer=Po(this.pointwiseInitializer),t.depthwiseRegularizer=sh(this.depthwiseRegularizer),t.pointwiseRegularizer=sh(this.pointwiseRegularizer),t.depthwiseConstraint=_a(this.depthwiseConstraint),t.pointwiseConstraint=_a(this.pointwiseConstraint),t}}Ih.className="SeparableConv";class Nh extends Ih{constructor(t){super(2,t)}}Nh.className="SeparableConv2D",n.registerClass(Nh);class Ah extends wh{constructor(t){super(1,t),Ah.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Sa(t.kernelSize,"number",1,1))throw new ra(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}Ah.className="Conv1D",n.registerClass(Ah);class Ch extends al{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return s((()=>{if(t=Jo(t),"channelsLast"===this.dataFormat){const e=oo(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return oo(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=oo(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return oo(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Ch.className="Cropping2D",n.registerClass(Ch);class zh extends al{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ja(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,va(Ba,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,e){return s((()=>{let e=Jo(t);const n=e.shape;if("channelsFirst"===this.dataFormat){e=mt(e,[0,2,3,1]);const t=this.size[0]*n[2],s=this.size[1]*n[3],i="nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s]);return mt(i,[0,3,1,2])}{const t=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}zh.className="UpSampling2D",n.registerClass(zh);class Dh extends bh{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=Wo(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Ra(t.depthwiseConstraint),this.depthwiseRegularizer=rh(t.depthwiseRegularizer)}build(t){if((t=Zo(t)).length<4)throw new ra(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new ra(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{let e=function(t,e,n=[1,1],i="valid",r,a){return s((()=>{null==r&&(r="channelsLast"),ja(r);let s=gh(t,r);if(4!==t.rank)throw new ra(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==e.rank)throw new ra(`depthwiseKernel is required to be 4-D, but is instead ${e.rank}-D`);return s=vt(s,e,n,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(s=mt(s,[0,3,1,2])),s}))}(t=Jo(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=yo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=Zo(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=dh(e,this.kernelSize[0],this.padding,this.strides[0]),r=dh(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Po(this.depthwiseInitializer),t.depthwiseRegularizer=sh(this.depthwiseRegularizer),t.depthwiseConstraint=_a(this.depthwiseRegularizer),t}}function $h(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new ra("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function Th(t,e,n,i=!1,r,a,o=!1,l=!1){return s((()=>{const u=e.shape.length;if(u<3)throw new ra(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(no(2,u));if(e=mt(e,h),null!=a)throw new aa("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===u-1&&(r=St(r,-1)),r=mt(r,h)),i&&(e=It(e,0),null!=r&&(r=It(r,0)));const c=[];let p,d=n;const f=e.shape[0],g=Nt(e);let m,y;null!=r&&(m=Nt(r));for(let e=0;e<f;++e){const n=g[e],i=s((()=>t(n,d)));if(null==r)p=i[0],d=i[1];else{const t=s((()=>{const t=m[e],n=Y(t).sub(t);return{output:i[0].mul(t).add(d[0].mul(n)),newStates:d.map(((e,s)=>i[1][s].mul(t).add(e.mul(n))))}}));p=t.output,d=t.newStates}l&&c.push(p)}if(l){y=At(c,1)}return[p,y,d]}))}Dh.className="DepthwiseConv2D",n.registerClass(Dh);class Eh extends al{constructor(t){let e;if(super(t),null==t.cell)throw new ra("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Ph({cells:t.cell}):t.cell,null==e.stateSize)throw new ra("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new el({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return no(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){Go(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,e){return s((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new aa("Constants support is not implemented in RNN yet.");Go(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new el({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new ra(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new el({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new sa("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new ra("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>D([s,t]))):this.states_=[D([s,this.cell.stateSize])];else if(null==t)M(this.states_),null!=this.keptStates&&(M(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>D([s,t]))):this.states_[0]=D([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new ra(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):M(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.arraysEqual(i.shape,a))throw new ra(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>B(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=$h(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new el({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof nl){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=null==e?null:e.mask,s=null==e?null:e.training;let i=null==e?null:e.initialState;t=Jo(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new ra(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=Th(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return s((()=>{let e=D(t.shape);return e=r(e,[1,2]),e=io(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?ho(e,[1,t]):e)):this.cell.stateSize>1?[ho(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Eh.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=xl(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Eh.className="RNN",n.registerClass(Eh);class Fh extends al{}class _h extends Fh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Ia(this.units,"units"),this.activation=Xu(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Wo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Wo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Wo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=rh(t.kernelRegularizer),this.recurrentRegularizer=rh(t.recurrentRegularizer),this.biasRegularizer=rh(t.biasRegularizer),this.kernelConstraint=Ra(t.kernelConstraint),this.recurrentConstraint=Ra(t.recurrentConstraint),this.biasConstraint=Ra(t.biasConstraint),this.dropout=to([1,eo([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=to([1,eo([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=Zo(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new ra(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let n=t[1];t=t[0];const s=null!=e.training&&e.training;let i;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Wh({ones:()=>Y(t),rate:this.dropout,training:s})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Wh({ones:()=>Y(n),rate:this.recurrentDropout,training:s}));const r=this.dropoutMask,o=this.recurrentDropoutMask;i=po(null!=r?a(t,r):t,this.kernel.read()),null!=this.bias&&(i=yo(i,this.bias.read())),null!=o&&(n=a(n,o));let l=u(i,po(n,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Zu(this.activation),useBias:this.useBias,kernelInitializer:Po(this.kernelInitializer),recurrentInitializer:Po(this.recurrentInitializer),biasInitializer:Po(this.biasInitializer),kernelRegularizer:sh(this.kernelRegularizer),recurrentRegularizer:sh(this.recurrentRegularizer),biasRegularizer:sh(this.biasRegularizer),activityRegularizer:sh(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}_h.className="SimpleRNNCell",n.registerClass(_h);class Lh extends Eh{constructor(t){t.cell=new _h(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(M(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(M(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return new t(e)}}Lh.className="SimpleRNN",n.registerClass(Lh);class Rh extends Fh{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new ra("GRUCell does not support reset_after parameter set to true.");this.units=t.units,Ia(this.units,"units"),this.activation=Xu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Xu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Wo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Wo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Wo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=rh(t.kernelRegularizer),this.recurrentRegularizer=rh(t.recurrentRegularizer),this.biasRegularizer=rh(t.biasRegularizer),this.kernelConstraint=Ra(t.kernelConstraint),this.recurrentConstraint=Ra(t.recurrentConstraint),this.biasConstraint=Ra(t.biasConstraint),this.dropout=to([1,eo([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=to([1,eo([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=Zo(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new ra(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const n=null!=e.training&&e.training;let s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Wh({ones:()=>Y(t),rate:this.dropout,training:n,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Wh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:3}));const i=this.dropoutMask,r=this.recurrentDropoutMask;let o,l,h;0<this.dropout&&this.dropout<1&&(t=a(t,i[0]));let c=po(t,this.kernel.read());this.useBias&&(c=yo(c,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,r[0]));const p=this.recurrentKernel.read(),[d,f]=Ct(p,[2*this.units,this.units],p.rank-1),g=po(s,d),[m,y,b]=Ct(c,3,c.rank-1),[w,k]=Ct(g,2,g.rank-1);o=this.recurrentActivation.apply(u(m,w)),l=this.recurrentActivation.apply(u(y,k));const x=po(a(l,s),f);h=this.activation.apply(u(b,x));const v=u(a(o,s),a(u(1,j(o)),h));return[v,v]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Zu(this.activation),recurrentActivation:Zu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Po(this.kernelInitializer),recurrentInitializer:Po(this.recurrentInitializer),biasInitializer:Po(this.biasInitializer),kernelRegularizer:sh(this.kernelRegularizer),recurrentRegularizer:sh(this.recurrentRegularizer),biasRegularizer:sh(this.biasRegularizer),activityRegularizer:sh(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}Rh.className="GRUCell",n.registerClass(Rh);class Mh extends Eh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Rh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(M(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(M(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Mh.className="GRU",n.registerClass(Mh);class Oh extends Fh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Ia(this.units,"units"),this.activation=Xu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Xu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Wo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Wo(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Wo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=rh(t.kernelRegularizer),this.recurrentRegularizer=rh(t.recurrentRegularizer),this.biasRegularizer=rh(t.biasRegularizer),this.kernelConstraint=Ra(t.kernelConstraint),this.recurrentConstraint=Ra(t.recurrentConstraint),this.biasConstraint=Ra(t.biasConstraint),this.dropout=to([1,eo([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=to([1,eo([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=Zo(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends vo{apply(e,s){const i=t.apply([n]),r=(new Io).apply([n]),a=t.apply([2*n]);return uo(uo(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return s((()=>{const n=null!=e.training&&e.training;if(3!==(t=t).length)throw new ra(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let s=t[1];const i=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Wh({ones:()=>Y(t),rate:this.dropout,training:n,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Wh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:4}));const r=this.dropoutMask,o=this.recurrentDropoutMask;let l,h,c,p;0<this.dropout&&this.dropout<1&&(t=a(t,r[0]));let d=po(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,o[0])),d=u(d,po(s,this.recurrentKernel.read())),this.useBias&&(d=yo(d,this.bias.read()));const[f,g,m,y]=Ct(d,4,d.rank-1);l=this.recurrentActivation.apply(f),h=this.recurrentActivation.apply(g),c=u(a(h,i),a(l,this.activation.apply(m))),p=this.recurrentActivation.apply(y);const b=a(p,this.activation.apply(c));return[b,b,c]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Zu(this.activation),recurrentActivation:Zu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Po(this.kernelInitializer),recurrentInitializer:Po(this.recurrentInitializer),biasInitializer:Po(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:sh(this.kernelRegularizer),recurrentRegularizer:sh(this.recurrentRegularizer),biasRegularizer:sh(this.biasRegularizer),activityRegularizer:sh(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Oh.className="LSTMCell",n.registerClass(Oh);class Bh extends Eh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Oh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(M(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(M(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Bh.className="LSTM",n.registerClass(Bh);class Ph extends Fh{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return s((()=>{let n=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(n.splice(0,t.stateSize.length)):s.push(n.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];n=s[a],r=0===a?[t[0]].concat(n):[r[0]].concat(n),r=o.call(r,e),i.push(r.slice(1))}n=[];for(const t of i.slice().reverse())n.push(...t);return[r[0]].concat(n)}))}build(t){let e;Go(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{Ha(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(xl(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return Qo(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}tl(e)}}function Wh(t){const{ones:e,rate:n,training:s=!1,count:i=1}=t,r=()=>bo(e(),n),a=()=>wo(r,e,s);if(!i||i<=1)return B(a().clone());return Array(i).fill(void 0).map(a).map((t=>B(t.clone())))}Ph.className="StackedRNNCells",n.registerClass(Ph);class Uh extends Eh{constructor(t){if(t.unroll)throw new aa("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new aa("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new el({ndim:5})]}call(t,e){return s((()=>{if(null!=this.cell.dropoutMask&&(M(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(M(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new ra("ConvRNN2D cell does not support constants");const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return s((()=>{const{stateSize:e}=this.cell,n=t.shape,s=this.computeSingleOutputShape(n),i=[s[0],...s.slice(2)],r=D(i);return Array.isArray(e)?Array(e.length).fill(r):[r]}))}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new sa("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new ra("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>D(r))):this.states_=[D(r)];else if(null==t)M(this.states_),null!=this.keptStates&&(M(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>D(r))):this.states_[0]=D(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new ra(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):M(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.arraysEqual(s.shape,i))throw new ra(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>B(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=dh(l,s[0],i,r[0],a[0]),c=dh(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Uh.className="ConvRNN2D";class Kh extends Oh{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,Ia(this.filters,"filters"),this.kernelSize=ph(n,2,"kernelSize"),this.kernelSize.forEach((t=>Ia(t,"kernelSize"))),this.strides=ph(s||1,2,"strides"),this.strides.forEach((t=>Ia(t,"strides"))),this.padding=i||"valid",Va(this.padding),this.dataFormat=r||"channelsLast",ja(this.dataFormat),this.dilationRate=ph(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>Ia(t,"dilationRate")))}build(t){var e;t=Zo(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new ra(`The channel dimension of the input should be defined. Found ${t[n]}`);const s=t[n],i=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",i,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const r=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",r,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;t=new((e=class extends vo{apply(t,e){return lo([n.apply([s]),$([s]),n.apply([2*s])])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return s((()=>{if(3!==t.length)throw new ra(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const n=e.training||!1,s=t[0],i=t[1],r=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Wh({ones:()=>Y(s),rate:this.dropout,training:n,count:4}));const o=this.dropoutMask,l=(t,e,n)=>e&&e[n]?a(e[n],t):t;let h=l(s,o,0),c=l(s,o,1),p=l(s,o,2),d=l(s,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Wh({ones:()=>Y(i),rate:this.recurrentDropout,training:n,count:4}));const f=this.recurrentDropoutMask;let g=l(i,f,0),m=l(i,f,1),y=l(i,f,2),b=l(i,f,3);const[w,k,x,v]=Ct(this.kernel.read(),4,3),[S,I,N,A]=this.useBias?Ct(this.bias.read(),4):[null,null,null,null];h=this.inputConv(h,w,S,this.padding),c=this.inputConv(c,k,I,this.padding),p=this.inputConv(p,x,N,this.padding),d=this.inputConv(d,v,A,this.padding);const[C,z,D,$]=Ct(this.recurrentKernel.read(),4,3);g=this.recurrentConv(g,C),m=this.recurrentConv(m,z),y=this.recurrentConv(y,D),b=this.recurrentConv(b,$);const T=this.recurrentActivation.apply(u(h,g)),E=this.recurrentActivation.apply(u(c,m)),F=u(a(E,r),a(T,this.activation.apply(u(p,y)))),_=a(this.recurrentActivation.apply(u(d,b)),this.activation.apply(F));return[_,_,F]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,e,n,s){const i=zt(t,e,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?yo(i,n,this.dataFormat):i}recurrentConv(t,e){return zt(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Kh.className="ConvLSTM2DCell",n.registerClass(Kh);class jh extends Uh{constructor(t){const e=new Kh(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}jh.className="ConvLSTM2D",n.registerClass(jh);class Vh extends al{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(n);return wo((()=>bo(n,this.rate,s,this.seed)),(()=>n),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}Vh.className="Dropout",n.registerClass(Vh);class qh extends Vh{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}qh.className="SpatialDropout1D",n.registerClass(qh);class Gh extends al{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,Ia(this.units,"units"),this.activation=Xu(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=Wo(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=Wo(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Ra(t.kernelConstraint),this.biasConstraint=Ra(t.biasConstraint),this.kernelRegularizer=rh(t.kernelRegularizer),this.biasRegularizer=rh(t.biasRegularizer),this.activityRegularizer=rh(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=Zo(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=Zo(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t),s=Aa(this.activation.getClassName());let i;return null!=s?i=po(n,this.kernel.read(),s,this.bias?this.bias.read():null):(i=po(n,this.kernel.read()),null!=this.bias&&(i=yo(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:Zu(this.activation),useBias:this.useBias,kernelInitializer:Po(this.kernelInitializer),biasInitializer:Po(this.biasInitializer),kernelRegularizer:sh(this.kernelRegularizer),biasRegularizer:sh(this.biasRegularizer),activityRegularizer:sh(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),biasConstraint:_a(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Gh.className="Dense",n.registerClass(Gh);class Hh extends al{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=Zo(t);for(const e of t.slice(1))if(null==e)throw new ra(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],Qa(t,1)]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=Jo(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const t=[0];for(let e=2;e<n.rank;++e)t.push(e);t.push(1),n=n.transpose(t)}return function(t){if(t.rank<=1)throw new ra(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],Qa(t.shape,1)];return t.reshape(e)}(n)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}Hh.className="Flatten",n.registerClass(Hh);class Jh extends al{constructor(t){super(t),this.supportsMasking=!0,this.activation=Xu(t.activation)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t);return this.activation.apply(n)}))}getConfig(){const t={activation:Zu(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}Jh.className="Activation",n.registerClass(Jh);class Zh extends al{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return s((()=>{return t=Jo(t),e=t,n=this.n,s((()=>{if(2!==e.shape.length)throw new ra(`repeat() expects a rank-2 tensor, but received a rank-${e.shape.length} tensor.`);return ho(io(e,1),[1,n,1])}));var e,n}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Zh.className="RepeatVector",n.registerClass(Zh);class Yh extends al{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new ra("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=Qa(t);if(null!==r){if(0===i||a%i!=0)throw new ra(n);s[r]=a/i}else if(a!==i)throw new ra(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t),s=n.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return n.reshape(i)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}Yh.className="Reshape",n.registerClass(Yh);class Xh extends al{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=no(1,t.dims.length+1);if(!e.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new el({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=Zo(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,e){return mt(Jo(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}Xh.className="Permute",n.registerClass(Xh);class Qh extends al{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=Jo(t);return Dt($t(n,this.maskValue),-1)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t),s=Dt($t(n,this.maskValue),-1,!0);return n.mul(s.asType(n.dtype))}))}}Qh.className="Masking",n.registerClass(Qh);class tc extends al{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(pa(t.inputLength))}this.inputDim=t.inputDim,Ia(this.inputDim,"inputDim"),this.outputDim=t.outputDim,Ia(this.outputDim,"outputDim"),this.embeddingsInitializer=Wo(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=rh(t.embeddingsRegularizer),this.activityRegularizer=rh(t.activityRegularizer),this.embeddingsConstraint=Ra(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return s((()=>this.maskZero?(t=Jo(t),$t(t,Tt(t))):null))}computeOutputShape(t){if(t=Zo(t),null==this.inputLength)return[...t,this.outputDim];const e=pa(this.inputLength);if(e.length!==t.length-1)throw new ra(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new ra(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=Jo(t);"int32"!==n.dtype&&(n=so(n,"int32"));return fo(this.embeddings.read(),n.as1D()).reshape(Zo(this.computeOutputShape(n.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Po(this.embeddingsInitializer),embeddingsRegularizer:sh(this.embeddingsRegularizer),activityRegularizer:sh(this.activityRegularizer),embeddingsConstraint:_a(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}tc.className="Embedding",n.registerClass(tc);class ec extends al{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new aa}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new ra("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[Zo(t)]),(t=t).length<2)throw new ra(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=ka(e),e.length>1)throw new ra(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===ka(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return s((()=>{if(t=t,this.reshapeRequired){const e=[],n=t.map((t=>t.rank));if(-1===n.indexOf(null)){const s=eo(n);for(let n of t){const t=n.rank;for(let e=0;e<s-t;++e)n=io(n,1);e.push(n)}return this.mergeFunction(e)}{let n=!1;for(const s of t){const t=s.rank;if(null==t){const t=s.shape,i=t[0],r=t.slice(1).concat([i]);let a=s.reshape([i].concat(Qa(t.slice(1))));a=mt(a,[1,0]),a=a.reshape(r),e.push(a),n=!0}else if(t>1){const i=no(1,t).concat([0]);e.push(mt(s,i)),n=!0}else e.push(s)}let s=this.mergeFunction(e);const i=s.rank;if(n)if(null==i){const t=s.shape,e=t[t.length-1],n=[e].concat(t.slice(0,t.length-1));s=mt(s.reshape([-1,e]),[1,0]).reshape(n)}else if(i>1){const t=[i-1].concat(no(0,i-1));s=mt(s,t)}return s}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=ka(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,e){return s((()=>{if(null==e)return null;if(!Array.isArray(e))throw new ra("`mask` should be an Array");if(!Array.isArray(t))throw new ra("`inputs` should be an Array");if(e.length!==t.length)throw new ra(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every((t=>null==t)))return null;let n=(e=e.map((t=>null==t?t:St(t,0))))[0];for(let t=1;t<e.length-1;++t)n=et(n,e[t]);return n}))}}class nc extends ec{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return e}))}}nc.className="Add",n.registerClass(nc);class sc extends ec{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=a(e,t[n]);return e}))}}sc.className="Multiply",n.registerClass(sc);class ic extends ec{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return a(1/t.length,e)}))}}ic.className="Average",n.registerClass(ic);class rc extends ec{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=q(e,t[n]);return e}))}}rc.className="Maximum",n.registerClass(rc);class ac extends ec{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=ht(e,t[n]);return e}))}}ac.className="Minimum",n.registerClass(ac);class oc extends ec{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new ra("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new ra("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return s((()=>lo(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new ra("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new ra("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new ra("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new ra(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return s((()=>{let n=!0;if(e.forEach((t=>{null==t||(n=!1)})),n)return null;const s=[];for(let n=0;n<t.length;++n)null==e[n]?s.push(Y(t[n]).asType("bool")):e[n].rank<t[n].rank?s.push(St(e[n],-1)):s.push(e[n]);const i=S(s,this.axis);return Et(i,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function lc(t,e){for(;t<0;)t+=e;return t}oc.className="Concatenate",n.registerClass(oc);class uc extends ec{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new aa("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new ra(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new ra(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,i=t[0],r=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>lc(e,t[n].shape.length))):[lc(this.axes,i.shape.length),lc(this.axes,r.shape.length)],this.normalize&&(i=vl(i,n[0]),r=vl(r,n[1])),function(t,n,i){if(t.shape.length>3||n.shape.length>3)throw new aa("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof i&&(i=[i,i]),"complex64"===t.dtype||"complex64"===n.dtype)throw new aa("batchDot is not implemented for complex64-type Tensors yet.");const r=t.shape.length,a=n.shape.length;null==i&&(i=[r-1,a-2]);const o=i;return s((()=>{let e,s;if(r>a){e=r-a;const t=[];for(let n=0;n<e;++n)t.push(1);n=n.reshape(n.shape.concat(t))}else if(a>r){e=a-r;const n=[];for(let t=0;t<e;++t)n.push(1);t=t.reshape(t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=o[0]===o[1]?t.mul(n).sum(o[0]):t.transpose([1,0]).mul(n).sum(o[1]);else{const e=o[0]!==t.shape.length-1,i=o[1]===n.shape.length-1;s=t.matMul(n,e,i)}if(e>0){let t;t=r>a?r+a-3:r-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=s.squeeze(n)}return 1===s.shape.length&&(s=s.expandDims(1)),s}))}(i,r,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[lc(this.axes,t.length),lc(this.axes,e.length)],n}computeOutputShape(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new aa("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}uc.className="Dot",n.registerClass(uc);class hc extends al{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t);return wo((()=>co(n.shape,0,this.stddev).add(n)),(()=>n),e.training||!1)}))}}hc.className="GaussianNoise",n.registerClass(hc);class cc extends al{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Jo(t);if(this.rate>0&&this.rate<1){return wo((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return n.mul(co(n.shape,1,t))}),(()=>n),e.training||!1)}return n}))}}cc.className="GaussianDropout",n.registerClass(cc);class pc extends al{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||Jo(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t);return wo((()=>{const e=Jo(t),s=-1.7580993408473766;let i=Ft(E(n),this.rate);i=so(i,"float32");const r=((1-this.rate)*(1+this.rate*s**2))**-.5,a=-r*s*this.rate;return e.mul(i).add(i.add(-1).mul(s)).mul(r).add(a)}),(()=>Jo(t)),e.training||!1)}return t}))}}function dc(t,e,n,s,i,r=.001){let a;if(2===t.rank)a=Lt(t,e,n,s,i,r);else if(3===t.rank)a=Rt(t,e,n,s,i,r);else{if(4!==t.rank)throw new aa(`batchNormalization is not implemented for array of rank ${t.rank} yet`);a=Mt(t,e,n,s,i,r)}return a}function fc(t,n,i,r,a=.001){return e.arraysEqual(r.slice().sort(),no(0,t.rank-1))?function(t,e,n,i,r=.001){return s((()=>{const s=_t(t,i),a=s.mean,o=s.variance;return[dc(t,a,o,n,e,r),a,o]}))}(t,n,i,r,a):function(t,e,n,i,r=.001){return s((()=>{const s=_t(t,i),a=s.mean,o=s.variance,l=[];for(const e of no(0,t.rank))-1!==i.indexOf(e)?l.push(1):l.push(t.shape[e]);const u=a.reshape(l),h=o.reshape(l),c=null==e?null:e.reshape(l),p=null==n?null:n.reshape(l);return[dc(t,u,h,p,c,r),a,o]}))}(t,n,i,r,a)}pc.className="AlphaDropout",n.registerClass(pc);class gc extends al{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Wo(t.betaInitializer||"zeros"),this.gammaInitializer=Wo(t.gammaInitializer||"ones"),this.movingMeanInitializer=Wo(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=Wo(t.movingVarianceInitializer||"ones"),this.betaConstraint=Ra(t.betaConstraint),this.gammaConstraint=Ra(t.gammaConstraint),this.betaRegularizer=rh(t.betaRegularizer),this.gammaRegularizer=rh(t.gammaRegularizer)}build(t){t=Zo(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new ra(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new el({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return s((()=>{const i=null!=n.training&&n.training,r=Jo(t),a=r.shape,o=a.length,l=no(0,o),u=this.axis>=0?this.axis:this.axis+o;l.splice(u,1);const h=la(1,o);h[u]=a[u];const c=l.slice();c.sort();const p=!e.arraysEqual(c,no(0,o).slice(0,o-1));if(!i)return(()=>{if(p){const t=this.movingMean.read().reshape(h),e=this.movingVariance.read().reshape(h),n=this.center?this.beta.read().reshape(h):null,s=this.scale?this.gamma.read().reshape(h):null;return dc(r,t,e,n,s,this.epsilon)}return dc(r,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[d,f,g]=fc(r,this.gamma.read(),this.beta.read(),l,this.epsilon),m=(t,e,n)=>{s((()=>{const s=1-n,i=t.read(),r=i.sub(e).mul(s);t.write(i.sub(r))}))};return(()=>{m(this.movingMean,f,this.momentum),m(this.movingVariance,g,this.momentum)})(),d}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Po(this.betaInitializer),gammaInitializer:Po(this.gammaInitializer),movingMeanInitializer:Po(this.movingMeanInitializer),movingVarianceInitializer:Po(this.movingVarianceInitializer),betaRegularizer:sh(this.betaRegularizer),gammaRegularizer:sh(this.gammaRegularizer),betaConstraint:_a(this.betaConstraint),gammaConstraint:_a(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}gc.className="BatchNormalization",n.registerClass(gc);class mc extends al{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Wo(t.betaInitializer||"zeros"),this.gammaInitializer=Wo(t.gammaInitializer||"ones"),this.betaRegularizer=rh(t.betaRegularizer),this.gammaRegularizer=rh(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=Zo(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==ka(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,e){const n=Jo(t),i=n.shape,r=i.length;return s((()=>{let{mean:t,variance:e}=_t(n,this.axis,!0);const s=la(1,r);for(const t of this.axis)s[t]=i[t];const a=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?t.reshape(s):t;let o=a(this.gamma.read()),l=a(this.beta.read());const u=[],h=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(u.push(i[t]),h.push(1)):(u.push(1),h.push(i[t]));return t=t.tile(u),e=e.tile(u),o=o.tile(h),l=l.tile(h),dc(n,t,e,l,o,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Po(this.betaInitializer),gammaInitializer:Po(this.gammaInitializer),betaRegularizer:sh(this.betaRegularizer),gammaRegularizer:sh(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}mc.className="LayerNormalization",n.registerClass(mc);class yc extends al{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new ra(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new ra(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new ra(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new el({ndim:4})]}computeOutputShape(t){let e,n;return t=Zo(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return s((()=>{return e=Jo(t),n=this.padding,i=this.dataFormat,s((()=>{if(4!==e.rank)throw new ra(`temporalPadding expects input tensor to be 4-D, but received a ${e.rank}-D tensor.`);if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new ra("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new ra(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],Ot(e,t)}));var e,n,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function bc(t,e,n,i,r,a){return s((()=>{let s;ja(r),qa(a),Va(i),null==n&&(n=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=gh(t,r);const o="same"===i?"same":"valid";return s="max"===a?Bt(t,e,n,o):Pt(t,e,n,o),"channelsFirst"===r&&(s=mt(s,[0,3,1,2])),s}))}function wc(t,e,n,i,r,a){return s((()=>{let s;ja(r),qa(a),Va(i),null==n&&(n=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=mh(t,r);const o="same"===i?"same":"valid";return s="max"===a?Wt(t,e,n,o):Ut(t,e,n,o),"channelsFirst"===r&&(s=mt(s,[0,4,1,2,3])),s}))}yc.className="ZeroPadding2D",n.registerClass(yc);class kc extends al{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new ra(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(Ia(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new ra(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}Ia(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,Va(this.padding),this.inputSpec=[new el({ndim:3})]}computeOutputShape(t){const e=dh((t=Zo(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return s((()=>{this.invokeCallHook(t,e),t=io(Jo(t),2);const n=this.poolingFunction(Jo(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return Kt(n,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class xc extends kc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),bc(t,e,n,s,i,"max")}}xc.className="MaxPooling1D",n.registerClass(xc);class vc extends kc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),bc(t,e,n,s,i,"avg")}}vc.className="AveragePooling1D",n.registerClass(vc);class Sc extends al{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new ra(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];Ia(this.poolSize,"poolSize"),Ia(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ja(this.dataFormat),Va(this.padding),this.inputSpec=[new el({ndim:4})]}computeOutputShape(t){t=Zo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=dh(e,this.poolSize[0],this.padding,this.strides[0]),n=dh(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(Jo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Ic extends Sc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),bc(t,e,n,s,i,"max")}}Ic.className="MaxPooling2D",n.registerClass(Ic);class Nc extends Sc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),bc(t,e,n,s,i,"avg")}}Nc.className="AveragePooling2D",n.registerClass(Nc);class Ac extends al{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new ra(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];Ia(this.poolSize,"poolSize"),Ia(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ja(this.dataFormat),Va(this.padding),this.inputSpec=[new el({ndim:5})]}computeOutputShape(t){t=Zo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=dh(e,this.poolSize[0],this.padding,this.strides[0]),n=dh(n,this.poolSize[1],this.padding,this.strides[1]),s=dh(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(Jo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Cc extends Ac{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),wc(t,e,n,s,i,"max")}}Cc.className="MaxPooling3D",n.registerClass(Cc);class zc extends Ac{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return ja(i),Va(s),wc(t,e,n,s,i,"avg")}}zc.className="AveragePooling3D",n.registerClass(zc);class Dc extends al{constructor(t){super(t),this.inputSpec=[new el({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new aa}}class $c extends Dc{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=Jo(t);return U(e,1)}))}}$c.className="GlobalAveragePooling1D",n.registerClass($c);class Tc extends Dc{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=Jo(t);return J(e,1)}))}}Tc.className="GlobalMaxPooling1D",n.registerClass(Tc);class Ec extends al{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,ja(this.dataFormat),this.inputSpec=[new el({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new aa}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Fc extends Ec{call(t,e){return s((()=>{const e=Jo(t);return"channelsLast"===this.dataFormat?U(e,[1,2]):U(e,[2,3])}))}}Fc.className="GlobalAveragePooling2D",n.registerClass(Fc);class _c extends Ec{call(t,e){return s((()=>{const e=Jo(t);return"channelsLast"===this.dataFormat?J(e,[1,2]):J(e,[2,3])}))}}_c.className="GlobalMaxPooling2D",n.registerClass(_c);class Lc extends al{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=xl(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class Rc extends Lc{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=Zo(t)).length<3)throw new ra(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=Zo(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,e){return s((()=>Th(((t,n)=>[Jo(this.layer.call(t,e)),[]]),t=Jo(t),[],!1,null,null,!1,!0)[1]))}}Rc.className="TimeDistributed",n.registerClass(Rc);class Mc extends Lc{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=xl(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=xl(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,va(Ua,"BidirectionalMergeMode",i),t.weights)throw new aa("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):ca(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=$h(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new ra("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new el({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new aa("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof nl;for(const t of r)if(t instanceof nl!==o)throw new ra("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=e.initialState;let s,i,r,o;if(null==n)s=this.forwardLayer.call(t,e),i=this.backwardLayer.call(t,e);else{const r=n.slice(0,n.length/2),a=n.slice(n.length/2);s=this.forwardLayer.call(t,Object.assign(e,{initialState:r})),i=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(s)&&(r=s.slice(1).concat(i.slice(1))),s=s[0],i=i[0]),this.returnSequences&&(i=It(i,1)),"concat"===this.mergeMode?o=lo([s,i]):"sum"===this.mergeMode?o=u(s,i):"ave"===this.mergeMode?o=a(.5,u(s,i)):"mul"===this.mergeMode?o=a(s,i):null==this.mergeMode&&(o=[s,i]),this.returnState?null==this.mergeMode?o.concat(r):[o].concat(r):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){Ha(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),Ha(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=xl(e.layer);if(delete e.layer,null!=e.numConstants)throw new aa("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Oc(t){return new vc(t)}function Bc(t){return new Nc(t)}function Pc(t){return new zc(t)}function Wc(t){return new Tc(t)}function Uc(t){return new _c(t)}function Kc(t){return new xc(t)}function jc(t){return new Ic(t)}Mc.className="Bidirectional",n.registerClass(Mc);const Vc=Wc,qc=Uc,Gc=Kc,Hc=jc;var Jc=Object.freeze({__proto__:null,inputLayer:function(t){return new ll(t)},elu:function(t){return new uh(t)},reLU:function(t){return new ah(t)},leakyReLU:function(t){return new oh(t)},prelu:function(t){return new lh(t)},softmax:function(t){return new ch(t)},thresholdedReLU:function(t){return new hh(t)},conv1d:function(t){return new Ah(t)},conv2d:function(t){return new kh(t)},conv2dTranspose:function(t){return new vh(t)},conv3d:function(t){return new xh(t)},conv3dTranspose:function(t){return new Sh(t)},separableConv2d:function(t){return new Nh(t)},cropping2D:function(t){return new Ch(t)},upSampling2d:function(t){return new zh(t)},depthwiseConv2d:function(t){return new Dh(t)},activation:function(t){return new Jh(t)},dense:function(t){return new Gh(t)},dropout:function(t){return new Vh(t)},spatialDropout1d:function(t){return new qh(t)},flatten:function(t){return new Hh(t)},repeatVector:function(t){return new Zh(t)},reshape:function(t){return new Yh(t)},permute:function(t){return new Xh(t)},embedding:function(t){return new tc(t)},add:function(t){return new nc(t)},average:function(t){return new ic(t)},concatenate:function(t){return new oc(t)},maximum:function(t){return new rc(t)},minimum:function(t){return new ac(t)},multiply:function(t){return new sc(t)},dot:function(t){return new uc(t)},batchNormalization:function(t){return new gc(t)},layerNormalization:function(t){return new mc(t)},zeroPadding2d:function(t){return new yc(t)},averagePooling1d:Oc,avgPool1d:function(t){return Oc(t)},avgPooling1d:function(t){return Oc(t)},averagePooling2d:Bc,avgPool2d:function(t){return Bc(t)},avgPooling2d:function(t){return Bc(t)},averagePooling3d:Pc,avgPool3d:function(t){return Pc(t)},avgPooling3d:function(t){return Pc(t)},globalAveragePooling1d:function(t){return new $c(t)},globalAveragePooling2d:function(t){return new Fc(t)},globalMaxPooling1d:Wc,globalMaxPooling2d:Uc,maxPooling1d:Kc,maxPooling2d:jc,maxPooling3d:function(t){return new Cc(t)},gru:function(t){return new Mh(t)},gruCell:function(t){return new Rh(t)},lstm:function(t){return new Bh(t)},lstmCell:function(t){return new Oh(t)},simpleRNN:function(t){return new Lh(t)},simpleRNNCell:function(t){return new _h(t)},convLstm2d:function(t){return new jh(t)},convLstm2dCell:function(t){return new Kh(t)},rnn:function(t){return new Eh(t)},stackedRNNCells:function(t){return new Ph(t)},bidirectional:function(t){return new Mc(t)},timeDistributed:function(t){return new Rc(t)},globalMaxPool1d:Vc,globalMaxPool2d:qc,maxPool1d:Gc,maxPool2d:Hc,Layer:al,RNN:Eh,RNNCell:Fh,input:Fu,gaussianNoise:function(t){return new hc(t)},gaussianDropout:function(t){return new cc(t)},alphaDropout:function(t){return new pc(t)},masking:function(t){return new Qh(t)}});var Zc=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return El(t,e)},binaryCrossentropy:function(t,e){return Ml(t,e)},sparseCategoricalAccuracy:function(t,e){return Ol(t,e)},categoricalAccuracy:function(t,e){return Fl(t,e)},categoricalCrossentropy:function(t,e){return Bl(t,e)},precision:function(t,e){return Ll(t,e)},recall:function(t,e){return Rl(t,e)},cosineProximity:function(t,e){return Dl(t,e)},meanAbsoluteError:function(t,e){return Il(t,e)},meanAbsolutePercentageError:function(t,e){return Nl(t,e)},MAPE:function(t,e){return Nl(t,e)},mape:function(t,e){return Nl(t,e)},meanSquaredError:function(t,e){return Sl(t,e)},MSE:function(t,e){return Sl(t,e)},mse:function(t,e){return Sl(t,e)}}),Yc=Object.freeze({__proto__:null,modelFromJSON:async function(t,e){"modelTopology"in t||(t={modelTopology:t});let n=(t=t).modelTopology;null!=n.model_config&&(n=n.model_config);const s=xl(Yl(n),e);if(null!=t.weightsManifest){const e=await lt.loadWeights(t.weightsManifest,t.pathPrefix,s.weights.map((t=>t.originalName))),n={};for(const t of s.weights)n[t.originalName]=e[t.originalName];s.loadWeights(n),M(e)}return s}});var Xc=Object.freeze({__proto__:null,l1l2:function(t){return new eh(t)},l1:function(t){return Qu(e=t),new eh({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return Qu(e=t),new eh({l2:null!=e?e.l2:null,l1:0});var e}});class Qc extends dl{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof Au))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function tp(t,e){return t<e}function ep(t,e){return t>e}class np extends Qc{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new aa("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=tp:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=ep:this.monitorFunc=tp,this.monitorFunc===tp&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===tp?1/0:-1/0}async onEpochEnd(t,e){await hl(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const sp={earlyStopping:function(t){return new np(t)}};export{Qc as Callback,fl as CallbackList,yl as CustomCallback,np as EarlyStopping,ml as History,el as InputSpec,Xo as LayerVariable,Au as LayersModel,Eh as RNN,Du as Sequential,nl as SymbolicTensor,sp as callbacks,Ma as constraints,Uo as initializers,Fu as input,Jc as layers,Eu as loadLayersModel,Zc as metrics,$u as model,Yc as models,_u as registerCallbackConstructor,Xc as regularizers,Tu as sequential,Ql as version_layers};
//# sourceMappingURL=tf-layers.fesm.min.js.map
